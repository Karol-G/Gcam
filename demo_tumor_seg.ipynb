{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "ROOT_DIR = \"data/tumor_seg_data\"\n",
    "TRANSFORM = False\n",
    "DEBUG = False\n",
    "\n",
    "class TumorDataset(Dataset):\n",
    "    \"\"\" Returns a TumorDataset class object which represents our tumor dataset.\n",
    "    TumorDataset inherits from torch.utils.data.Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device):\n",
    "        \"\"\" Constructor for our TumorDataset class.\n",
    "        Parameters:\n",
    "            root_dir(str): Directory with all the images.\n",
    "            transform(bool): Flag to apply image random transformation.\n",
    "            DEBUG(bool): To switch to debug mode for image transformation.\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.root_dir = ROOT_DIR\n",
    "        self.transform = {'hflip': TF.hflip,\n",
    "                          'vflip': TF.vflip,\n",
    "                          'rotate': TF.rotate}\n",
    "        self.default_transformation = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((512, 512))\n",
    "        ])\n",
    "        self.DEBUG = DEBUG\n",
    "        if not TRANSFORM:\n",
    "            self.transform = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Overridden method from inheritted class to support\n",
    "        indexing of dataset such that datset[I] can be used\n",
    "        to get Ith sample.\n",
    "        Parameters:\n",
    "            index(int): Index of the dataset sample\n",
    "\n",
    "        Return:\n",
    "            sample(dict): Contains the index, image, mask torch.Tensor.\n",
    "                        'index': Index of the image.\n",
    "                        'image': Contains the tumor image torch.Tensor.\n",
    "                        'mask' : Contains the mask image torch.Tensor.\n",
    "        \"\"\"\n",
    "        index += 3000\n",
    "        image_name = os.path.join(self.root_dir, str(index)+'.png')\n",
    "        mask_name = os.path.join(self.root_dir, str(index)+'_mask.png')\n",
    "\n",
    "        image = Image.open(image_name)\n",
    "        mask = Image.open(mask_name)\n",
    "\n",
    "        image = self.default_transformation(image)\n",
    "        mask = self.default_transformation(mask)\n",
    "\n",
    "        # Custom transformations\n",
    "        if self.transform:\n",
    "            image, mask = self._random_transform(image, mask)\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "        #mask = TF.to_tensor(mask).squeeze()\n",
    "\n",
    "        image = image.to(self.device)\n",
    "        sample = {\"img\": image, \"gt\": mask, \"filepath\": image_name}\n",
    "        return sample\n",
    "\n",
    "    def _random_transform(self, image, mask):\n",
    "        \"\"\" Applies a set of transformation in random order.\n",
    "        Each transformation has a probability of 0.5\n",
    "        \"\"\"\n",
    "        choice_list = list(self.transform)\n",
    "        for _ in range(len(choice_list)):\n",
    "            choice_key = random.choice(choice_list)\n",
    "            if self.DEBUG:\n",
    "                print(f'Transform choose: {choice_key}')\n",
    "            action_prob = random.randint(0, 1)\n",
    "            if action_prob >= 0.5:\n",
    "                if self.DEBUG:\n",
    "                    print(f'\\tApplying transformation: {choice_key}')\n",
    "                if choice_key == 'rotate':\n",
    "                    rotation = random.randint(15, 75)\n",
    "                    if self.DEBUG:\n",
    "                        print(f'\\t\\tRotation by: {rotation}')\n",
    "                    image = self.transform[choice_key](image, rotation)\n",
    "                    mask = self.transform[choice_key](mask, rotation)\n",
    "                else:\n",
    "                    image = self.transform[choice_key](image)\n",
    "                    mask = self.transform[choice_key](mask)\n",
    "            choice_list.remove(choice_key)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Overridden method from inheritted class so that\n",
    "        len(self) returns the size of the dataset.\n",
    "        \"\"\"\n",
    "        error_msg = 'Part of dataset is missing!\\nNumber of tumor and mask images are not same.'\n",
    "        total_files = len(os.listdir(self.root_dir))\n",
    "\n",
    "        assert (total_files % 2 == 0), error_msg\n",
    "        return total_files//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from models.tumor_seg.bts.model import DynamicUNet\n",
    "\n",
    "FILTER_LIST = [16,32,64,128,256]\n",
    "MODEL_NAME = f\"UNet-{FILTER_LIST}.pt\"\n",
    "STATE_DICT_PATH = 'models/tumor_seg/saved_models/'\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "class TumorSegModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super(TumorSegModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.model = DynamicUNet(FILTER_LIST).to(self.device)\n",
    "        self.restore_model(os.path.join(STATE_DICT_PATH, MODEL_NAME))\n",
    "        self.model.eval()\n",
    "\n",
    "    def restore_model(self, path):\n",
    "        \"\"\" Loads the saved model and restores it to the \"model\" object.\n",
    "        Loads the model based on device used for computation.(CPU/GPU)\n",
    "        Follows the best method recommended by Pytorch\n",
    "        Link: https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended\n",
    "        Parameters:\n",
    "            path(str): The file location where the model is saved.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.device == 'cpu':\n",
    "            self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load(path))\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        self.output = self.model(batch)\n",
    "        self.mask = (self.output > THRESHOLD)\n",
    "        self.output = self.output * self.mask\n",
    "        self.output[self.output != 0] = 1\n",
    "\n",
    "        output_numpy = self.output.detach().cpu().numpy()\n",
    "        self.classes, self.is_ok = [], []\n",
    "        for x in output_numpy:\n",
    "            nonzero = np.count_nonzero(x)\n",
    "            if nonzero > 0:\n",
    "                self.classes.append([\"tumor\"])\n",
    "                self.is_ok.append(True)\n",
    "            else:\n",
    "                self.classes.append([\"no_tumor\"])\n",
    "                self.is_ok.append(False)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "\n",
    "    def is_backward_ready(self):\n",
    "        return True\n",
    "\n",
    "    def get_mask(self):\n",
    "        return self.mask\n",
    "\n",
    "    def get_ok_list(self):\n",
    "        return self.is_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | Progress: 1.5625% | Finished in: 0d 0h 0m 1s | Time Per Batch: 0.02s\n",
      "Dismissed the last 28 module layers (Note: This number can be inflated if the model contains many nested module layers)\n",
      "Selected module layer: \n",
      "overlap_percentage: 73.04%\n",
      "avg_overlap_percentage: 73.04%\n",
      "Iteration: 1 | Progress: 3.125% | Finished in: 0d 0h 6m 21s | Time Per Batch: 6.14s\n",
      "Dismissed the last 28 module layers (Note: This number can be inflated if the model contains many nested module layers)\n",
      "Selected module layer: \n",
      "overlap_percentage: 61.31%\n",
      "avg_overlap_percentage: 67.18%\n",
      "Iteration: 2 | Progress: 4.6875% | Finished in: 0d 0h 8m 53s | Time Per Batch: 8.74s\n",
      "Dismissed the last 28 module layers (Note: This number can be inflated if the model contains many nested module layers)\n",
      "Selected module layer: \n",
      "overlap_percentage: 4.76%\n",
      "avg_overlap_percentage: 46.37%\n",
      "Iteration: 3 | Progress: 6.25% | Finished in: 0d 0h 9m 21s | Time Per Batch: 9.34s\n",
      "Dismissed the last 28 module layers (Note: This number can be inflated if the model contains many nested module layers)\n",
      "Selected module layer: \n",
      "overlap_percentage: 1.07%\n",
      "avg_overlap_percentage: 35.04%\n",
      "Iteration: 4 | Progress: 7.8125% | Finished in: 0d 0h 9m 20s | Time Per Batch: 9.49s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-20a7e087658e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTumorSegModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mevaluate_grad_cam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished evaluation.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitKraken\\cnn_interpretability\\evaluate_grad_cam.py\u001b[0m in \u001b[0;36mevaluate_dataset\u001b[1;34m(model, dataset)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mis_ok\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Only if object are detected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mmodel_GBP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[0mattention_map_GBP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_GBP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mmodel_GCAM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitKraken\\cnn_interpretability\\evaluation_models\\grad_cam\\grad_cam.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_backward_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_category_id_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\grad-cam\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\grad-cam\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import evaluate_grad_cam\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "dataset = TumorDataset(device=DEVICE)\n",
    "model = TumorSegModel(device=DEVICE)\n",
    "\n",
    "evaluate_grad_cam.evaluate_dataset(model, dataset)\n",
    "print(\"Finished evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
