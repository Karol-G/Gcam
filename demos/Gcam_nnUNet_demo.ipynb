{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gcam nnUNet demo",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h00Piy5ERmK",
        "colab_type": "text"
      },
      "source": [
        "# **Using Gcam with the nnUNet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS3PKZ3YMy47",
        "colab_type": "text"
      },
      "source": [
        "In this demo you will learn how to use Gcam with the nnUNet to extract 3D attention maps. We will use the [Medical Segmentation Decathlon](http://medicaldecathlon.com/) 3D prostate dataset as an example. The nnUNet splits the input data into patches and reconstructs them afterwards over the course of multiple classes. As a consequence the attention maps generated by Gcam will also only be patches which need to be reconstructed afterwards. This will make the usage of Gcam a little bit more complicated but not by much as you will see. \\\\\n",
        "\n",
        "This demonstration was made using Google Colab and probably won't work if you are not using Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVSx7IY2EIFw",
        "colab_type": "text"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkl2JV-BNSzc",
        "colab_type": "text"
      },
      "source": [
        "Clone and install the nnUNet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ClKFrxHNaAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/MIC-DKFZ/nnUNet.git\n",
        "%cd nnUNet\n",
        "!git reset --hard b38c69b345b2f60cd0d053039669e8f988b0c0af # Reset repo to a specific commit as nnUNet code changes often. This ensures that the demo will work.\n",
        "!pip install -e ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj7BnbeH_orr",
        "colab_type": "text"
      },
      "source": [
        "Install gdown to download files from google drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyaoz58m_u3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install gdown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJHguxcd_zx9",
        "colab_type": "text"
      },
      "source": [
        "Download the prostate dataset from google drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3VhL4UN_6-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /content/nnUNet_raw_data_base/nnUNet_raw_data\n",
        "!mkdir -p /content/nnUNet_raw_data_base/nnUNet_preprocessed\n",
        "!mkdir -p /content/nnUNet_trained_models\n",
        "%cd /content/nnUNet_raw_data_base/nnUNet_raw_data\n",
        "!gdown https://drive.google.com/uc?id=1Ff7c21UksxyT4JfETjaarmuKEjdqe1-a\n",
        "!tar -xvf Task05_Prostate.tar\n",
        "!rm Task05_Prostate.tar\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svpC5N96armV",
        "colab_type": "text"
      },
      "source": [
        "Set environment variables for the nnUNet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrMOpj1Mm8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%env nnUNet_raw_data_base=/content/nnUNet_raw_data_base/nnUNet_raw_data\n",
        "%env nnUNet_preprocessed=/content/nnUNet_raw_data_base/nnUNet_preprocessed\n",
        "%env RESULTS_FOLDER=/content/nnUNet_trained_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjD3A5G5a8wq",
        "colab_type": "text"
      },
      "source": [
        "Convert and preprocess the dataset (this might take some time):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X32Pml4uH5hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nnUNet_convert_decathlon_task -i /content/nnUNet_raw_data_base/nnUNet_raw_data/Task05_Prostate -p 1\n",
        "!nnUNet_plan_and_preprocess -t 005 --verify_dataset_integrity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19aAeFNaayqU",
        "colab_type": "text"
      },
      "source": [
        "Download the pretrained model for the prostate dataset (this might take some time):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MeFqguQL37R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nnUNet_download_pretrained_model Task005_Prostate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrzgksboVuk_",
        "colab_type": "text"
      },
      "source": [
        "Install gcam:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFW0O0n-VxQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install gcam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf8HMvn6EiQs",
        "colab_type": "text"
      },
      "source": [
        "# Injecting Gcam into nnUNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EGy67IisvqQ",
        "colab_type": "text"
      },
      "source": [
        "Create a directory for the gcam evaluation results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY9XZyDJi7Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/gcam_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCDQzyl_lTGf",
        "colab_type": "text"
      },
      "source": [
        "To inject nnUNet with Gcam we need to modify the predict.py inside `nnUNet/nnunet/inference`. This is done with the following code which needs to be inserted into the file:\n",
        "\n",
        "```\n",
        "# Line 34\n",
        "from gcam import gcam\n",
        "# Line 188\n",
        "trainer.network = gcam.inject(trainer.network, channels=3, replace=True, backend=\"gcam\", layer='seg_outputs.5', postprocessor=torch.nn.Softmax(dim=2), label=lambda x: 0.5 < x)\n",
        "```\n",
        "\n",
        "Herby the parameters of `gcam.inject` have the following meaning: \\\\\n",
        "`channels=3`: nnUNet encodes the different classes it is able to predict/segment in the channel dimension. We need to tell gcam that the attention maps should have the same number of channels/classes as well. \\\\\n",
        "`replace=True`: We also tell gcam that it should return the attention maps patches whenever the model forward() is called instead of the normal prediction/segmentation. As a result nnUNet will reconstruct our patches to full attention maps for us.  \\\\\n",
        "`backend=\"gcam\"`: The backend which we want to use. In this case Grad-CAM. \\\\\n",
        "`layer='seg_outputs.5'`: The layer of interest we want to generate attention maps from. \\\\\n",
        "`postprocessor=torch.nn.Softmax(dim=2)`: Applies internally the softmax function to each channel of the model prediction which is needed to generate the attention maps correctly. \\\\\n",
        "`label=lambda x: 0.5 < x`: Thresholds the model prediction after the postprocessor is applied. \n",
        "\n",
        "All the possible parameters of `gcam.inject` are explained in the documentation as well. Especially the documentation for the parameters `postprocessor` and `label` should be read for a better understandig.\n",
        "\n",
        "\n",
        "Gcam has also the ability to evaluate the attention maps if the corresponding ground truth masks are given. For completion, the code needed for the evaluation is shown below:\n",
        "\n",
        "```\n",
        "# Line 34\n",
        "from gcam import gcam\n",
        "# Line 188\n",
        "evaluator = gcam.Evaluator(\"/content/gcam_results\")\n",
        "# Line 189\n",
        "trainer.network = gcam.inject(trainer.network, channels=3, replace=True, backend=\"gcam\", layer='seg_outputs.5', postprocessor=torch.nn.Softmax(dim=2), label=lambda x: 0.5 < x) \n",
        "# Original inference code here...\n",
        "# ...\n",
        "# ...\n",
        "# Evaluation after inference:\n",
        "# Line 244 (Replace with the original)\n",
        "filenames = [i.get() for i in results]\n",
        "# Line 246\n",
        "evaluate(evaluator, filenames, layer)\n",
        "# Line 247\n",
        "evaluator.dump()\n",
        "# Line 274-285\n",
        "def evaluate(evaluator, filenames, layer):\n",
        "    filenames = np.asarray(filenames).squeeze()\n",
        "    for filename in filenames:\n",
        "        attention_map = np.array(nib.load(filename).dataobj)\n",
        "        mask_name = filename[-21:]\n",
        "        mask_name = mask_name[:11] + mask_name[-7:]\n",
        "        class_label = int(filename[-8])\n",
        "        mask = np.array(nib.load(\"/content/nnUNet_raw_data_base/nnUNet_raw_data/nnUNet_raw_data/Task005_Prostate/labelsTr/\" + mask_name).dataobj)\n",
        "        mask[mask != class_label] = -1\n",
        "        mask[mask == class_label] = 1\n",
        "        mask[mask != 1] = 0\n",
        "        evaluator.comp_score(attention_map, mask, layer=layer, class_label=class_label, name=mask_name)\n",
        "\n",
        "```\n",
        "\n",
        "*Note: Because we are returning the attention maps instead of the nnUNet predictions the evaluation cannot be done by Gcam internally. Instead we have to use Gcam's external evaluator which results in more code than usual for the evaluation to work.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsmah0PfnKhp",
        "colab_type": "text"
      },
      "source": [
        "The following cell includes the already modified predict.py. By running the cell once the original predict.py will be replaced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3Ya0ke6gTOf",
        "colab_type": "code",
        "outputId": "8739a19b-1510-4c26-f56f-d0068e05d00e",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Modified predict.py - Replaces original predict.py\n",
        "%%writefile /content/nnUNet/nnunet/inference/predict.py\n",
        "#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany\n",
        "#\n",
        "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#    you may not use this file except in compliance with the License.\n",
        "#    You may obtain a copy of the License at\n",
        "#\n",
        "#        http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#    Unless required by applicable law or agreed to in writing, software\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#    See the License for the specific language governing permissions and\n",
        "#    limitations under the License.\n",
        "\n",
        "\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "from batchgenerators.augmentations.utils import resize_segmentation\n",
        "from nnunet.inference.segmentation_export import save_segmentation_nifti_from_softmax, save_segmentation_nifti\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "from multiprocessing import Process, Queue\n",
        "import torch\n",
        "import SimpleITK as sitk\n",
        "import shutil\n",
        "from multiprocessing import Pool\n",
        "from nnunet.postprocessing.connected_components import load_remove_save, load_postprocessing\n",
        "from nnunet.training.model_restore import load_model_and_checkpoint_files\n",
        "from nnunet.training.network_training.nnUNetTrainer import nnUNetTrainer\n",
        "from nnunet.utilities.one_hot_encoding import to_one_hot\n",
        "from gcam import gcam\n",
        "\n",
        "\n",
        "def preprocess_save_to_queue(preprocess_fn, q, list_of_lists, output_files, segs_from_prev_stage, classes, transpose_forward):\n",
        "    # suppress output\n",
        "    #sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    errors_in = []\n",
        "    for i, l in enumerate(list_of_lists):\n",
        "        try:\n",
        "            output_file = output_files[i]\n",
        "            print(\"preprocessing\", output_file)\n",
        "            d, _, dct = preprocess_fn(l)\n",
        "            # print(output_file, dct)\n",
        "            if segs_from_prev_stage[i] is not None:\n",
        "                assert isfile(segs_from_prev_stage[i]) and segs_from_prev_stage[i].endswith(\n",
        "                    \".nii.gz\"), \"segs_from_prev_stage\" \\\n",
        "                                \" must point to a \" \\\n",
        "                                \"segmentation file\"\n",
        "                seg_prev = sitk.GetArrayFromImage(sitk.ReadImage(segs_from_prev_stage[i]))\n",
        "                # check to see if shapes match\n",
        "                img = sitk.GetArrayFromImage(sitk.ReadImage(l[0]))\n",
        "                assert all([i == j for i, j in zip(seg_prev.shape, img.shape)]), \"image and segmentation from previous \" \\\n",
        "                                                                                 \"stage don't have the same pixel array \" \\\n",
        "                                                                                 \"shape! image: %s, seg_prev: %s\" % \\\n",
        "                                                                                 (l[0], segs_from_prev_stage[i])\n",
        "                seg_prev = seg_prev.transpose(transpose_forward)\n",
        "                seg_reshaped = resize_segmentation(seg_prev, d.shape[1:], order=1, cval=0)\n",
        "                seg_reshaped = to_one_hot(seg_reshaped, classes)\n",
        "                d = np.vstack((d, seg_reshaped)).astype(np.float32)\n",
        "            \"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n",
        "            larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n",
        "            communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n",
        "            enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n",
        "            patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n",
        "            then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n",
        "            filename or np.ndarray and will handle this automatically\"\"\"\n",
        "            print(d.shape)\n",
        "            if np.prod(d.shape) > (2e9 / 4 * 0.85):  # *0.85 just to be save, 4 because float32 is 4 bytes\n",
        "                print(\n",
        "                    \"This output is too large for python process-process communication. \"\n",
        "                    \"Saving output temporarily to disk\")\n",
        "                np.save(output_file[:-7] + \".npy\", d)\n",
        "                d = output_file[:-7] + \".npy\"\n",
        "            q.put((output_file, (d, dct)))\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt\n",
        "        except Exception as e:\n",
        "            print(\"error in\", l)\n",
        "            print(e)\n",
        "    q.put(\"end\")\n",
        "    if len(errors_in) > 0:\n",
        "        print(\"There were some errors in the following cases:\", errors_in)\n",
        "        print(\"These cases were ignored.\")\n",
        "    else:\n",
        "        print(\"This worker has ended successfully, no errors to report\")\n",
        "    # restore output\n",
        "    #sys.stdout = sys.__stdout__\n",
        "\n",
        "\n",
        "def preprocess_multithreaded(trainer, list_of_lists, output_files, num_processes=2, segs_from_prev_stage=None):\n",
        "    if segs_from_prev_stage is None:\n",
        "        segs_from_prev_stage = [None] * len(list_of_lists)\n",
        "\n",
        "    num_processes = min(len(list_of_lists), num_processes)\n",
        "\n",
        "    classes = list(range(1, trainer.num_classes))\n",
        "    assert isinstance(trainer, nnUNetTrainer)\n",
        "    q = Queue(2)\n",
        "    processes = []\n",
        "    for i in range(num_processes):\n",
        "        pr = Process(target=preprocess_save_to_queue, args=(trainer.preprocess_patient, q,\n",
        "                                                         list_of_lists[i::num_processes],\n",
        "                                                         output_files[i::num_processes],\n",
        "                                                         segs_from_prev_stage[i::num_processes],\n",
        "                                                         classes, trainer.plans['transpose_forward']))\n",
        "        pr.start()\n",
        "        processes.append(pr)\n",
        "\n",
        "    try:\n",
        "        end_ctr = 0\n",
        "        while end_ctr != num_processes:\n",
        "            item = q.get()\n",
        "            if item == \"end\":\n",
        "                end_ctr += 1\n",
        "                continue\n",
        "            else:\n",
        "                yield item\n",
        "\n",
        "    finally:\n",
        "        for p in processes:\n",
        "            if p.is_alive():\n",
        "                p.terminate()  # this should not happen but better safe than sorry right\n",
        "            p.join()\n",
        "\n",
        "        q.close()\n",
        "\n",
        "\n",
        "def predict_cases(model, list_of_lists, output_filenames, folds, save_npz, num_threads_preprocessing,\n",
        "                  num_threads_nifti_save, segs_from_prev_stage=None, do_tta=True, fp16=None, overwrite_existing=False,\n",
        "                  all_in_gpu=False, step_size=0.5, force_separate_z=None, interp_order=3, interp_order_z=0,\n",
        "                  checkpoint_name=\"model_final_checkpoint\"):\n",
        "    \"\"\"\n",
        "\n",
        "    :param model:\n",
        "    :param list_of_lists:\n",
        "    :param output_filenames:\n",
        "    :param folds:\n",
        "    :param save_npz:\n",
        "    :param num_threads_preprocessing:\n",
        "    :param num_threads_nifti_save:\n",
        "    :param segs_from_prev_stage:\n",
        "    :param do_tta:\n",
        "    :param overwrite_existing:\n",
        "    :param fp16: if None then we take no action. If True/False we overwrite what the model has in its init\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    assert len(list_of_lists) == len(output_filenames)\n",
        "    if segs_from_prev_stage is not None: assert len(segs_from_prev_stage) == len(output_filenames)\n",
        "\n",
        "    pool = Pool(num_threads_nifti_save)\n",
        "    results = []\n",
        "\n",
        "    cleaned_output_files = []\n",
        "    for o in output_filenames:\n",
        "        dr, f = os.path.split(o)\n",
        "        if len(dr) > 0:\n",
        "            maybe_mkdir_p(dr)\n",
        "        if not f.endswith(\".nii.gz\"):\n",
        "            f, _ = os.path.splitext(f)\n",
        "            f = f + \".nii.gz\"\n",
        "        cleaned_output_files.append(join(dr, f))\n",
        "\n",
        "    if not overwrite_existing:\n",
        "        print(\"number of cases:\", len(list_of_lists))\n",
        "        not_done_idx = [i for i, j in enumerate(cleaned_output_files) if not isfile(j)]\n",
        "\n",
        "        cleaned_output_files = [cleaned_output_files[i] for i in not_done_idx]\n",
        "        list_of_lists = [list_of_lists[i] for i in not_done_idx]\n",
        "        if segs_from_prev_stage is not None:\n",
        "            segs_from_prev_stage = [segs_from_prev_stage[i] for i in not_done_idx]\n",
        "\n",
        "        print(\"number of cases that still need to be predicted:\", len(cleaned_output_files))\n",
        "\n",
        "    print(\"emptying cuda cache\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"loading parameters for folds,\", folds)\n",
        "    trainer, params = load_model_and_checkpoint_files(model, folds, fp16=fp16, checkpoint_name=checkpoint_name)\n",
        "\n",
        "    print(\"starting preprocessing generator\")\n",
        "    preprocessing = preprocess_multithreaded(trainer, list_of_lists, cleaned_output_files, num_threads_preprocessing,\n",
        "                                             segs_from_prev_stage)\n",
        "\n",
        "    evaluator = gcam.Evaluator(\"/content/gcam_results\")\n",
        "    trainer.network = gcam.inject(trainer.network, output_dir=\"/content/gcam_results\", channels=3, replace=True, backend=\"gcam\", layer='seg_outputs.5', postprocessor=torch.nn.Softmax(dim=2), label=lambda x: 0.5 < x)\n",
        "\n",
        "    print(\"starting prediction...\")\n",
        "    all_output_files = []\n",
        "    for preprocessed in preprocessing:\n",
        "        output_filename, (d, dct) = preprocessed\n",
        "        all_output_files.append(all_output_files)\n",
        "        if isinstance(d, str):\n",
        "            data = np.load(d)\n",
        "            os.remove(d)\n",
        "            d = data\n",
        "\n",
        "        print(\"predicting\", output_filename)\n",
        "        softmax = []\n",
        "        for p in params:\n",
        "            trainer.load_checkpoint_ram(p, False)\n",
        "            softmax.append(trainer.predict_preprocessed_data_return_seg_and_softmax(\n",
        "                d, do_tta, trainer.data_aug_params['mirror_axes'], True, step_size=step_size, use_gaussian=True,\n",
        "                all_in_gpu=all_in_gpu)[1][None])\n",
        "\n",
        "        softmax = np.vstack(softmax)\n",
        "        softmax_mean = np.mean(softmax, 0)\n",
        "\n",
        "        transpose_forward = trainer.plans.get('transpose_forward')\n",
        "        if transpose_forward is not None:\n",
        "            transpose_backward = trainer.plans.get('transpose_backward')\n",
        "            softmax_mean = softmax_mean.transpose([0] + [i + 1 for i in transpose_backward])\n",
        "\n",
        "        if save_npz:\n",
        "            npz_file = output_filename[:-7] + \".npz\"\n",
        "        else:\n",
        "            npz_file = None\n",
        "\n",
        "        \"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n",
        "        larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n",
        "        communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n",
        "        enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n",
        "        patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n",
        "        then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n",
        "        filename or np.ndarray and will handle this automatically\"\"\"\n",
        "        bytes_per_voxel = 4\n",
        "        if all_in_gpu:\n",
        "            bytes_per_voxel = 2 # if all_in_gpu then the return value is half (float16)\n",
        "        if np.prod(softmax_mean.shape) > (2e9 / bytes_per_voxel * 0.85):  # * 0.85 just to be save\n",
        "            print(\n",
        "                \"This output is too large for python process-process communication. Saving output temporarily to disk\")\n",
        "            np.save(output_filename[:-7] + \".npy\", softmax_mean)\n",
        "            softmax_mean = output_filename[:-7] + \".npy\"\n",
        "\n",
        "        results.append(pool.starmap_async(save_segmentation_nifti_from_softmax,\n",
        "                                          ((softmax_mean, output_filename, dct, interp_order, None, None, None,\n",
        "                                            npz_file, None, force_separate_z, interp_order_z),)\n",
        "                                          ))\n",
        "\n",
        "    print(\"inference done. Now waiting for the segmentation export to finish...\")\n",
        "    filenames = [i.get() for i in results]\n",
        "\n",
        "    evaluate(evaluator, filenames, layer)\n",
        "    evaluator.dump()\n",
        "\n",
        "\n",
        "    # now apply postprocessing\n",
        "    # first load the postprocessing properties if they are present. Else raise a well visible warning\n",
        "    results = []\n",
        "    pp_file = join(model, \"postprocessing.json\")\n",
        "    if isfile(pp_file):\n",
        "        print(\"postprocessing...\")\n",
        "        shutil.copy(pp_file, os.path.abspath(os.path.dirname(output_filenames[0])))\n",
        "        # for_which_classes stores for which of the classes everything but the largest connected component needs to be\n",
        "        # removed\n",
        "        for_which_classes, min_valid_obj_size = load_postprocessing(pp_file)\n",
        "        results.append(pool.starmap_async(load_remove_save,\n",
        "                                          zip(output_filenames, output_filenames,\n",
        "                                              [for_which_classes] * len(output_filenames),\n",
        "                                              [min_valid_obj_size] * len(output_filenames))))\n",
        "        _ = [i.get() for i in results]\n",
        "    else:\n",
        "        print(\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"\n",
        "              \"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"\n",
        "              \"%s\" % model)\n",
        "\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "\n",
        "def evaluate(evaluator, filenames, layer):\n",
        "    filenames = np.asarray(filenames).squeeze()\n",
        "    for filename in filenames:\n",
        "        attention_map = np.array(nib.load(filename).dataobj)\n",
        "        mask_name = filename[-21:]\n",
        "        mask_name = mask_name[:11] + mask_name[-7:]\n",
        "        class_label = int(filename[-8])\n",
        "        mask = np.array(nib.load(\"/content/nnUNet_raw_data_base/nnUNet_raw_data/nnUNet_raw_data/Task005_Prostate/labelsTr/\" + mask_name).dataobj)\n",
        "        mask[mask != class_label] = -1\n",
        "        mask[mask == class_label] = 1\n",
        "        mask[mask != 1] = 0\n",
        "        evaluator.comp_score(attention_map, mask, layer=layer, class_label=class_label, name=mask_name)\n",
        "\n",
        "\n",
        "def predict_cases_fast(model, list_of_lists, output_filenames, folds, num_threads_preprocessing,\n",
        "                       num_threads_nifti_save, segs_from_prev_stage=None, do_tta=True, fp16=None,\n",
        "                       overwrite_existing=False, all_in_gpu=True, step_size=0.5, checkpoint_name=\"model_final_checkpoint\",\n",
        "                       force_separate_z=None, interp_order=3):\n",
        "    assert len(list_of_lists) == len(output_filenames)\n",
        "    if segs_from_prev_stage is not None: assert len(segs_from_prev_stage) == len(output_filenames)\n",
        "\n",
        "    pool = Pool(num_threads_nifti_save)\n",
        "    results = []\n",
        "\n",
        "    cleaned_output_files = []\n",
        "    for o in output_filenames:\n",
        "        dr, f = os.path.split(o)\n",
        "        if len(dr) > 0:\n",
        "            maybe_mkdir_p(dr)\n",
        "        if not f.endswith(\".nii.gz\"):\n",
        "            f, _ = os.path.splitext(f)\n",
        "            f = f + \".nii.gz\"\n",
        "        cleaned_output_files.append(join(dr, f))\n",
        "\n",
        "    if not overwrite_existing:\n",
        "        print(\"number of cases:\", len(list_of_lists))\n",
        "        not_done_idx = [i for i, j in enumerate(cleaned_output_files) if not isfile(j)]\n",
        "\n",
        "        cleaned_output_files = [cleaned_output_files[i] for i in not_done_idx]\n",
        "        list_of_lists = [list_of_lists[i] for i in not_done_idx]\n",
        "        if segs_from_prev_stage is not None:\n",
        "            segs_from_prev_stage = [segs_from_prev_stage[i] for i in not_done_idx]\n",
        "\n",
        "        print(\"number of cases that still need to be predicted:\", len(cleaned_output_files))\n",
        "\n",
        "    print(\"emptying cuda cache\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"loading parameters for folds,\", folds)\n",
        "    trainer, params = load_model_and_checkpoint_files(model, folds, fp16=fp16, checkpoint_name=checkpoint_name)\n",
        "\n",
        "    print(\"starting preprocessing generator\")\n",
        "    preprocessing = preprocess_multithreaded(trainer, list_of_lists, cleaned_output_files, num_threads_preprocessing,\n",
        "                                             segs_from_prev_stage)\n",
        "\n",
        "    print(\"starting prediction...\")\n",
        "    for preprocessed in preprocessing:\n",
        "        print(\"getting data from preprocessor\")\n",
        "        output_filename, (d, dct) = preprocessed\n",
        "        print(\"got something\")\n",
        "        if isinstance(d, str):\n",
        "            print(\"what I got is a string, so I need to load a file\")\n",
        "            data = np.load(d)\n",
        "            os.remove(d)\n",
        "            d = data\n",
        "\n",
        "        # preallocate the output arrays\n",
        "        # same dtype as the return value in predict_preprocessed_data_return_seg_and_softmax (saves time)\n",
        "        softmax_aggr = None # np.zeros((trainer.num_classes, *d.shape[1:]), dtype=np.float16)\n",
        "        all_seg_outputs = np.zeros((len(params), *d.shape[1:]), dtype=int)\n",
        "        print(\"predicting\", output_filename)\n",
        "\n",
        "        for i, p in enumerate(params):\n",
        "            trainer.load_checkpoint_ram(p, False)\n",
        "\n",
        "            res = trainer.predict_preprocessed_data_return_seg_and_softmax(\n",
        "                d, do_tta, trainer.data_aug_params['mirror_axes'], True, step_size=step_size, use_gaussian=True,\n",
        "                all_in_gpu=all_in_gpu)[1]\n",
        "\n",
        "            if len(params) > 1:\n",
        "                # otherwise we dont need this and we can save ourselves the time it takes to copy that\n",
        "                print(\"aggregating softmax\")\n",
        "                if softmax_aggr is None:\n",
        "                    softmax_aggr = res[1]\n",
        "                else:\n",
        "                    softmax_aggr += res[1]\n",
        "            all_seg_outputs[i] = res[0]\n",
        "\n",
        "        print(\"obtaining segmentation map\")\n",
        "        if len(params) > 1:\n",
        "            # we dont need to normalize the softmax by 1 / len(params) because this would not change the outcome of the argmax\n",
        "            seg = softmax_aggr.argmax(0)\n",
        "        else:\n",
        "            seg = all_seg_outputs[0]\n",
        "\n",
        "        print(\"applying transpose_backward\")\n",
        "        transpose_forward = trainer.plans.get('transpose_forward')\n",
        "        if transpose_forward is not None:\n",
        "            transpose_backward = trainer.plans.get('transpose_backward')\n",
        "            seg = seg.transpose([i for i in transpose_backward])\n",
        "\n",
        "        print(\"initializing segmentation export\")\n",
        "        results.append(pool.starmap_async(save_segmentation_nifti,\n",
        "                                           ((seg, output_filename, dct, interp_order, force_separate_z),)\n",
        "                                           ))\n",
        "        print(\"done\")\n",
        "\n",
        "    print(\"inference done. Now waiting for the segmentation export to finish...\")\n",
        "    _ = [i.get() for i in results]\n",
        "    # now apply postprocessing\n",
        "    # first load the postprocessing properties if they are present. Else raise a well visible warning\n",
        "    results = []\n",
        "    pp_file = join(model, \"postprocessing.json\")\n",
        "    if isfile(pp_file):\n",
        "        print(\"postprocessing...\")\n",
        "        shutil.copy(pp_file, os.path.dirname(output_filenames[0]))\n",
        "        # for_which_classes stores for which of the classes everything but the largest connected component needs to be\n",
        "        # removed\n",
        "        for_which_classes, min_valid_obj_size = load_postprocessing(pp_file)\n",
        "        results.append(pool.starmap_async(load_remove_save,\n",
        "                                          zip(output_filenames, output_filenames,\n",
        "                                              [for_which_classes] * len(output_filenames),\n",
        "                                              [min_valid_obj_size] * len(output_filenames))))\n",
        "        _ = [i.get() for i in results]\n",
        "    else:\n",
        "        print(\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"\n",
        "              \"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"\n",
        "              \"%s\" % model)\n",
        "\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "\n",
        "def predict_cases_fastest(model, list_of_lists, output_filenames, folds, num_threads_preprocessing,\n",
        "                          num_threads_nifti_save, segs_from_prev_stage=None, do_tta=True, fp16=None,\n",
        "                          overwrite_existing=False, all_in_gpu=True, step_size=0.5,\n",
        "                          checkpoint_name=\"model_final_checkpoint\"):\n",
        "    assert len(list_of_lists) == len(output_filenames)\n",
        "    if segs_from_prev_stage is not None: assert len(segs_from_prev_stage) == len(output_filenames)\n",
        "\n",
        "    pool = Pool(num_threads_nifti_save)\n",
        "    results = []\n",
        "\n",
        "    cleaned_output_files = []\n",
        "    for o in output_filenames:\n",
        "        dr, f = os.path.split(o)\n",
        "        if len(dr) > 0:\n",
        "            maybe_mkdir_p(dr)\n",
        "        if not f.endswith(\".nii.gz\"):\n",
        "            f, _ = os.path.splitext(f)\n",
        "            f = f + \".nii.gz\"\n",
        "        cleaned_output_files.append(join(dr, f))\n",
        "\n",
        "    if not overwrite_existing:\n",
        "        print(\"number of cases:\", len(list_of_lists))\n",
        "        not_done_idx = [i for i, j in enumerate(cleaned_output_files) if not isfile(j)]\n",
        "\n",
        "        cleaned_output_files = [cleaned_output_files[i] for i in not_done_idx]\n",
        "        list_of_lists = [list_of_lists[i] for i in not_done_idx]\n",
        "        if segs_from_prev_stage is not None:\n",
        "            segs_from_prev_stage = [segs_from_prev_stage[i] for i in not_done_idx]\n",
        "\n",
        "        print(\"number of cases that still need to be predicted:\", len(cleaned_output_files))\n",
        "\n",
        "    print(\"emptying cuda cache\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"loading parameters for folds,\", folds)\n",
        "    trainer, params = load_model_and_checkpoint_files(model, folds, fp16=fp16, checkpoint_name=checkpoint_name)\n",
        "\n",
        "    print(\"starting preprocessing generator\")\n",
        "    preprocessing = preprocess_multithreaded(trainer, list_of_lists, cleaned_output_files, num_threads_preprocessing,\n",
        "                                             segs_from_prev_stage)\n",
        "\n",
        "    print(\"starting prediction...\")\n",
        "    for preprocessed in preprocessing:\n",
        "        print(\"getting data from preprocessor\")\n",
        "        output_filename, (d, dct) = preprocessed\n",
        "        print(\"got something\")\n",
        "        if isinstance(d, str):\n",
        "            print(\"what I got is a string, so I need to load a file\")\n",
        "            data = np.load(d)\n",
        "            os.remove(d)\n",
        "            d = data\n",
        "\n",
        "        # preallocate the output arrays\n",
        "        # same dtype as the return value in predict_preprocessed_data_return_seg_and_softmax (saves time)\n",
        "        all_softmax_outputs = np.zeros((len(params), trainer.num_classes, *d.shape[1:]), dtype=np.float16)\n",
        "        all_seg_outputs = np.zeros((len(params), *d.shape[1:]), dtype=int)\n",
        "        print(\"predicting\", output_filename)\n",
        "\n",
        "        for i, p in enumerate(params):\n",
        "            trainer.load_checkpoint_ram(p, False)\n",
        "            res = trainer.predict_preprocessed_data_return_seg_and_softmax(\n",
        "                d, do_tta, trainer.data_aug_params['mirror_axes'], True, step_size=step_size, use_gaussian=True,\n",
        "                all_in_gpu=all_in_gpu\n",
        "            )[1]\n",
        "            if len(params) > 1:\n",
        "                # otherwise we dont need this and we can save ourselves the time it takes to copy that\n",
        "                all_softmax_outputs[i] = res[1]\n",
        "            all_seg_outputs[i] = res[0]\n",
        "\n",
        "        print(\"aggregating predictions\")\n",
        "        if len(params) > 1:\n",
        "            softmax_mean = np.mean(all_softmax_outputs, 0)\n",
        "            seg = softmax_mean.argmax(0)\n",
        "        else:\n",
        "            seg = all_seg_outputs[0]\n",
        "\n",
        "        print(\"applying transpose_backward\")\n",
        "        transpose_forward = trainer.plans.get('transpose_forward')\n",
        "        if transpose_forward is not None:\n",
        "            transpose_backward = trainer.plans.get('transpose_backward')\n",
        "            seg = seg.transpose([i for i in transpose_backward])\n",
        "\n",
        "        print(\"initializing segmentation export\")\n",
        "        results.append(pool.starmap_async(save_segmentation_nifti,\n",
        "                                           ((seg, output_filename, dct, 0, None),)\n",
        "                                           ))\n",
        "        print(\"done\")\n",
        "\n",
        "    print(\"inference done. Now waiting for the segmentation export to finish...\")\n",
        "    _ = [i.get() for i in results]\n",
        "    # now apply postprocessing\n",
        "    # first load the postprocessing properties if they are present. Else raise a well visible warning\n",
        "    results = []\n",
        "    pp_file = join(model, \"postprocessing.json\")\n",
        "    if isfile(pp_file):\n",
        "        print(\"postprocessing...\")\n",
        "        shutil.copy(pp_file, os.path.dirname(output_filenames[0]))\n",
        "        # for_which_classes stores for which of the classes everything but the largest connected component needs to be\n",
        "        # removed\n",
        "        for_which_classes, min_valid_obj_size = load_postprocessing(pp_file)\n",
        "        results.append(pool.starmap_async(load_remove_save,\n",
        "                                          zip(output_filenames, output_filenames,\n",
        "                                              [for_which_classes] * len(output_filenames),\n",
        "                                              [min_valid_obj_size] * len(output_filenames))))\n",
        "        _ = [i.get() for i in results]\n",
        "    else:\n",
        "        print(\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"\n",
        "              \"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"\n",
        "              \"%s\" % model)\n",
        "\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "\n",
        "def check_input_folder_and_return_caseIDs(input_folder, expected_num_modalities):\n",
        "    print(\"This model expects %d input modalities for each image\" % expected_num_modalities)\n",
        "    files = subfiles(input_folder, suffix=\".nii.gz\", join=False, sort=True)\n",
        "\n",
        "    maybe_case_ids = np.unique([i[:-12] for i in files])\n",
        "\n",
        "    remaining = deepcopy(files)\n",
        "    missing = []\n",
        "\n",
        "    assert len(files) > 0, \"input folder did not contain any images (expected to find .nii.gz file endings)\"\n",
        "\n",
        "    # now check if all required files are present and that no unexpected files are remaining\n",
        "    for c in maybe_case_ids:\n",
        "        for n in range(expected_num_modalities):\n",
        "            expected_output_file = c + \"_%04.0d.nii.gz\" % n\n",
        "            if not isfile(join(input_folder, expected_output_file)):\n",
        "                missing.append(expected_output_file)\n",
        "            else:\n",
        "                remaining.remove(expected_output_file)\n",
        "\n",
        "    print(\"Found %d unique case ids, here are some examples:\" % len(maybe_case_ids), np.random.choice(maybe_case_ids, min(len(maybe_case_ids), 10)))\n",
        "    print(\"If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc\")\n",
        "\n",
        "    if len(remaining) > 0:\n",
        "        print(\"found %d unexpected remaining files in the folder. Here are some examples:\" % len(remaining), np.random.choice(remaining, min(len(remaining), 10)))\n",
        "\n",
        "    if len(missing) > 0:\n",
        "        print(\"Some files are missing:\")\n",
        "        print(missing)\n",
        "        raise RuntimeError(\"missing files in input_folder\")\n",
        "\n",
        "    return maybe_case_ids\n",
        "\n",
        "\n",
        "def predict_from_folder(model, input_folder, output_folder, folds, save_npz, num_threads_preprocessing,\n",
        "                        num_threads_nifti_save, lowres_segmentations, part_id, num_parts, tta, fp16=False,\n",
        "                        overwrite_existing=True, mode='normal', overwrite_all_in_gpu=None, step_size: float = 0.5,\n",
        "                        force_separate_z=None, interp_order=3, interp_order_z=0, \n",
        "                        checkpoint_name=\"model_final_checkpoint\"):\n",
        "    \"\"\"\n",
        "        here we use the standard naming scheme to generate list_of_lists and output_files needed by predict_cases\n",
        "\n",
        "    :param model:\n",
        "    :param input_folder:\n",
        "    :param output_folder:\n",
        "    :param folds:\n",
        "    :param save_npz:\n",
        "    :param num_threads_preprocessing:\n",
        "    :param num_threads_nifti_save:\n",
        "    :param lowres_segmentations:\n",
        "    :param part_id:\n",
        "    :param num_parts:\n",
        "    :param tta:\n",
        "    :param fp16:\n",
        "    :param overwrite_existing: if not None then it will be overwritten with whatever is in there. None is default (no overwrite)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    maybe_mkdir_p(output_folder)\n",
        "    shutil.copy(join(model, 'plans.pkl'), output_folder)\n",
        "\n",
        "    assert isfile(join(model, \"plans.pkl\")), \"Folder with saved model weights must contain a plans.pkl file\"\n",
        "    expected_num_modalities = load_pickle(join(model, \"plans.pkl\"))['num_modalities']\n",
        "\n",
        "    # check input folder integrity\n",
        "    case_ids = check_input_folder_and_return_caseIDs(input_folder, expected_num_modalities)\n",
        "\n",
        "    output_files = [join(output_folder, i + \".nii.gz\") for i in case_ids]\n",
        "    all_files = subfiles(input_folder, suffix=\".nii.gz\", join=False, sort=True)\n",
        "    list_of_lists = [[join(input_folder, i) for i in all_files if i[:len(j)].startswith(j) and\n",
        "                      len(i) == (len(j) + 12)] for j in case_ids]\n",
        "\n",
        "    if lowres_segmentations is not None:\n",
        "        assert isdir(lowres_segmentations), \"if lowres_segmentations is not None then it must point to a directory\"\n",
        "        lowres_segmentations = [join(lowres_segmentations, i + \".nii.gz\") for i in case_ids]\n",
        "        assert all([isfile(i) for i in lowres_segmentations]), \"not all lowres_segmentations files are present. \" \\\n",
        "                                                               \"(I was searching for case_id.nii.gz in that folder)\"\n",
        "        lowres_segmentations = lowres_segmentations[part_id::num_parts]\n",
        "    else:\n",
        "        lowres_segmentations = None\n",
        "\n",
        "    if mode == \"normal\":\n",
        "        if overwrite_all_in_gpu is None:\n",
        "            all_in_gpu = False\n",
        "        else:\n",
        "            all_in_gpu = overwrite_all_in_gpu\n",
        "\n",
        "        return predict_cases(model, list_of_lists[part_id::num_parts], output_files[part_id::num_parts], folds,\n",
        "                             save_npz,\n",
        "                             num_threads_preprocessing, num_threads_nifti_save, lowres_segmentations,\n",
        "                             tta, fp16=fp16, overwrite_existing=overwrite_existing, all_in_gpu=all_in_gpu, step_size=step_size,\n",
        "                             force_separate_z=force_separate_z, interp_order=interp_order, interp_order_z=interp_order_z,\n",
        "                             checkpoint_name=checkpoint_name)\n",
        "    elif mode == \"fast\":\n",
        "        if overwrite_all_in_gpu is None:\n",
        "            all_in_gpu = True\n",
        "        else:\n",
        "            all_in_gpu = overwrite_all_in_gpu\n",
        "\n",
        "        assert save_npz is False\n",
        "        return predict_cases_fast(model, list_of_lists[part_id::num_parts], output_files[part_id::num_parts], folds,\n",
        "                                  num_threads_preprocessing, num_threads_nifti_save, lowres_segmentations,\n",
        "                                  tta, fp16=fp16, overwrite_existing=overwrite_existing, all_in_gpu=all_in_gpu, step_size=step_size,\n",
        "                                  force_separate_z=force_separate_z, interp_order=interp_order, \n",
        "                                  checkpoint_name=checkpoint_name)\n",
        "    elif mode == \"fastest\":\n",
        "        if overwrite_all_in_gpu is None:\n",
        "            all_in_gpu = True\n",
        "        else:\n",
        "            all_in_gpu = overwrite_all_in_gpu\n",
        "\n",
        "        assert save_npz is False\n",
        "        return predict_cases_fastest(model, list_of_lists[part_id::num_parts], output_files[part_id::num_parts], folds,\n",
        "                                     num_threads_preprocessing, num_threads_nifti_save, lowres_segmentations,\n",
        "                                     tta, fp16=fp16, overwrite_existing=overwrite_existing, all_in_gpu=all_in_gpu, \n",
        "                                     step_size=step_size, checkpoint_name=checkpoint_name)\n",
        "    else:\n",
        "        raise ValueError(\"unrecognized mode. Must be normal, fast or fastest\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-i\", '--input_folder', help=\"Must contain all modalities for each patient in the correct\"\n",
        "                                                     \" order (same as training). Files must be named \"\n",
        "                                                     \"CASENAME_XXXX.nii.gz where XXXX is the modality \"\n",
        "                                                     \"identifier (0000, 0001, etc)\", required=True)\n",
        "    parser.add_argument('-o', \"--output_folder\", required=True, help=\"folder for saving predictions\")\n",
        "    parser.add_argument('-m', '--model_output_folder',\n",
        "                        help='model output folder. Will automatically discover the folds '\n",
        "                             'that were '\n",
        "                             'run and use those as an ensemble', required=True)\n",
        "    parser.add_argument('-f', '--folds', nargs='+', default='None', help=\"folds to use for prediction. Default is None \"\n",
        "                                                                         \"which means that folds will be detected \"\n",
        "                                                                         \"automatically in the model output folder\")\n",
        "    parser.add_argument('-z', '--save_npz', required=False, action='store_true', help=\"use this if you want to ensemble\"\n",
        "                                                                                      \" these predictions with those of\"\n",
        "                                                                                      \" other models. Softmax \"\n",
        "                                                                                      \"probabilities will be saved as \"\n",
        "                                                                                      \"compresed numpy arrays in \"\n",
        "                                                                                      \"output_folder and can be merged \"\n",
        "                                                                                      \"between output_folders with \"\n",
        "                                                                                      \"merge_predictions.py\")\n",
        "    parser.add_argument('-l', '--lowres_segmentations', required=False, default='None', help=\"if model is the highres \"\n",
        "                                                                                             \"stage of the cascade then you need to use -l to specify where the segmentations of the \"\n",
        "                                                                                             \"corresponding lowres unet are. Here they are required to do a prediction\")\n",
        "    parser.add_argument(\"--part_id\", type=int, required=False, default=0, help=\"Used to parallelize the prediction of \"\n",
        "                                                                               \"the folder over several GPUs. If you \"\n",
        "                                                                               \"want to use n GPUs to predict this \"\n",
        "                                                                               \"folder you need to run this command \"\n",
        "                                                                               \"n times with --part_id=0, ... n-1 and \"\n",
        "                                                                               \"--num_parts=n (each with a different \"\n",
        "                                                                               \"GPU (for example via \"\n",
        "                                                                               \"CUDA_VISIBLE_DEVICES=X)\")\n",
        "    parser.add_argument(\"--num_parts\", type=int, required=False, default=1,\n",
        "                        help=\"Used to parallelize the prediction of \"\n",
        "                             \"the folder over several GPUs. If you \"\n",
        "                             \"want to use n GPUs to predict this \"\n",
        "                             \"folder you need to run this command \"\n",
        "                             \"n times with --part_id=0, ... n-1 and \"\n",
        "                             \"--num_parts=n (each with a different \"\n",
        "                             \"GPU (via \"\n",
        "                             \"CUDA_VISIBLE_DEVICES=X)\")\n",
        "    parser.add_argument(\"--num_threads_preprocessing\", required=False, default=6, type=int, help=\n",
        "    \"Determines many background processes will be used for data preprocessing. Reduce this if you \"\n",
        "    \"run into out of memory (RAM) problems. Default: 6\")\n",
        "    parser.add_argument(\"--num_threads_nifti_save\", required=False, default=2, type=int, help=\n",
        "    \"Determines many background processes will be used for segmentation export. Reduce this if you \"\n",
        "    \"run into out of memory (RAM) problems. Default: 2\")\n",
        "    parser.add_argument(\"--tta\", required=False, type=int, default=1, help=\"Set to 0 to disable test time data \"\n",
        "                                                                           \"augmentation (speedup of factor \"\n",
        "                                                                           \"4(2D)/8(3D)), \"\n",
        "                                                                           \"lower quality segmentations\")\n",
        "    parser.add_argument(\"--fp16\", required=False, help=\"Flag for inference in FP16, default = off. DO NOT USE! It \"\n",
        "                                                       \"doesn't work\", action=\"store_true\")\n",
        "    parser.add_argument(\"--overwrite_existing\", required=False, type=int, default=1, help=\"Set this to 0 if you need \"\n",
        "                                                                                          \"to resume a previous \"\n",
        "                                                                                          \"prediction. Default: 1 \"\n",
        "                                                                                          \"(=existing segmentations \"\n",
        "                                                                                          \"in output_folder will be \"\n",
        "                                                                                          \"overwritten)\")\n",
        "    parser.add_argument(\"--mode\", type=str, default=\"normal\", required=False)\n",
        "    parser.add_argument(\"--all_in_gpu\", type=str, default=\"None\", required=False, help=\"can be None, False or True\")\n",
        "    parser.add_argument(\"--step_size\", type=float, default=0.5, required=False, help=\"don't touch\")\n",
        "    parser.add_argument(\"--interp_order\", required=False, default=3, type=int,\n",
        "                        help=\"order of interpolation for segmentations, has no effect if mode=fastest\")\n",
        "    parser.add_argument(\"--interp_order_z\", required=False, default=0, type=int,\n",
        "                        help=\"order of interpolation along z is z is done differently\")\n",
        "    parser.add_argument(\"--force_separate_z\", required=False, default=\"None\", type=str,\n",
        "                        help=\"force_separate_z resampling. Can be None, True or False, has no effect if mode=fastest\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    input_folder = args.input_folder\n",
        "    output_folder = args.output_folder\n",
        "    part_id = args.part_id\n",
        "    num_parts = args.num_parts\n",
        "    model = args.model_output_folder\n",
        "    folds = args.folds\n",
        "    save_npz = args.save_npz\n",
        "    lowres_segmentations = args.lowres_segmentations\n",
        "    num_threads_preprocessing = args.num_threads_preprocessing\n",
        "    num_threads_nifti_save = args.num_threads_nifti_save\n",
        "    tta = args.tta\n",
        "    fp16 = args.fp16\n",
        "    step_size = args.step_size\n",
        "\n",
        "    interp_order = args.interp_order\n",
        "    interp_order_z = args.interp_order_z\n",
        "    force_separate_z = args.force_separate_z\n",
        "\n",
        "    if force_separate_z == \"None\":\n",
        "        force_separate_z = None\n",
        "    elif force_separate_z == \"False\":\n",
        "        force_separate_z = False\n",
        "    elif force_separate_z == \"True\":\n",
        "        force_separate_z = True\n",
        "    else:\n",
        "        raise ValueError(\"force_separate_z must be None, True or False. Given: %s\" % force_separate_z)\n",
        "\n",
        "    if fp16:\n",
        "        raise RuntimeError(\"FP16 support for inference does not work yet. Sorry :-/\")\n",
        "\n",
        "    overwrite = args.overwrite_existing\n",
        "    mode = args.mode\n",
        "    all_in_gpu = args.all_in_gpu\n",
        "\n",
        "    if lowres_segmentations == \"None\":\n",
        "        lowres_segmentations = None\n",
        "\n",
        "    if isinstance(folds, list):\n",
        "        if folds[0] == 'all' and len(folds) == 1:\n",
        "            pass\n",
        "        else:\n",
        "            folds = [int(i) for i in folds]\n",
        "    elif folds == \"None\":\n",
        "        folds = None\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for argument folds\")\n",
        "\n",
        "    if tta == 0:\n",
        "        tta = False\n",
        "    elif tta == 1:\n",
        "        tta = True\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for tta, Use 1 or 0\")\n",
        "\n",
        "    if overwrite == 0:\n",
        "        overwrite = False\n",
        "    elif overwrite == 1:\n",
        "        overwrite = True\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for overwrite, Use 1 or 0\")\n",
        "\n",
        "    assert all_in_gpu in ['None', 'False', 'True']\n",
        "    if all_in_gpu == \"None\":\n",
        "        all_in_gpu = None\n",
        "    elif all_in_gpu == \"True\":\n",
        "        all_in_gpu = True\n",
        "    elif all_in_gpu == \"False\":\n",
        "        all_in_gpu = False\n",
        "\n",
        "    predict_from_folder(model, input_folder, output_folder, folds, save_npz, num_threads_preprocessing,\n",
        "                        num_threads_nifti_save, lowres_segmentations, part_id, num_parts, tta, fp16=fp16,\n",
        "                        overwrite_existing=overwrite, mode=mode, overwrite_all_in_gpu=all_in_gpu, step_size=step_size,\n",
        "                        force_separate_z=force_separate_z, interp_order=interp_order, interp_order_z=interp_order_z)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/nnUNet/nnunet/inference/predict.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbPFx1OuqiFK",
        "colab_type": "text"
      },
      "source": [
        "*Note: If you use `gbp` as backend you additionally need to comment out the following line in nnUNetTrainer.py in `nnUNet/nnunet/training/network_training` by running the next cell:*\n",
        "\n",
        "```\n",
        "# Line 264\n",
        "# Comment out this line of code\n",
        "self.network.inference_apply_nonlin = softmax_helper\n",
        "```\n",
        "\n",
        "*`self.network.inference_apply_nonlin` is a nonlinearity that is applied to the output after each prediction and destroys the attention map when using Guided Backpropagation. None of the other backends have this problem.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjaX77W_sMbv",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "c662738a-4242-4e6d-e1fa-12a807a58695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Modified nnUNetTrainer.py - Replaces original nnUNetTrainer.py\n",
        "%%writefile /content/nnUNet/nnunet/training/network_training/nnUNetTrainer.py\n",
        "\n",
        "#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany\n",
        "#\n",
        "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#    you may not use this file except in compliance with the License.\n",
        "#    You may obtain a copy of the License at\n",
        "#\n",
        "#        http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#    Unless required by applicable law or agreed to in writing, software\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#    See the License for the specific language governing permissions and\n",
        "#    limitations under the License.\n",
        "\n",
        "\n",
        "import shutil\n",
        "from collections import OrderedDict\n",
        "from multiprocessing import Pool\n",
        "from time import sleep\n",
        "from typing import Tuple, List\n",
        "\n",
        "import matplotlib\n",
        "import nnunet\n",
        "import numpy as np\n",
        "import torch\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "from nnunet.configuration import default_num_threads\n",
        "from nnunet.evaluation.evaluator import aggregate_scores\n",
        "from nnunet.inference.segmentation_export import save_segmentation_nifti_from_softmax\n",
        "from nnunet.network_architecture.generic_UNet import Generic_UNet\n",
        "from nnunet.network_architecture.initialization import InitWeights_He\n",
        "from nnunet.network_architecture.neural_network import SegmentationNetwork\n",
        "from nnunet.postprocessing.connected_components import determine_postprocessing\n",
        "from nnunet.training.data_augmentation.default_data_augmentation import default_3D_augmentation_params, \\\n",
        "    default_2D_augmentation_params, get_default_augmentation, get_patch_size\n",
        "from nnunet.training.dataloading.dataset_loading import load_dataset, DataLoader3D, DataLoader2D, unpack_dataset\n",
        "from nnunet.training.loss_functions.dice_loss import DC_and_CE_loss\n",
        "from nnunet.training.network_training.network_trainer import NetworkTrainer\n",
        "from nnunet.utilities.nd_softmax import softmax_helper\n",
        "from nnunet.utilities.tensor_utilities import sum_tensor\n",
        "from torch import nn\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "matplotlib.use(\"agg\")\n",
        "\n",
        "try:\n",
        "    from apex.parallel import DistributedDataParallel as DDP\n",
        "except ImportError:\n",
        "    DDP = None\n",
        "\n",
        "\n",
        "class nnUNetTrainer(NetworkTrainer):\n",
        "    def __init__(self, plans_file, fold, output_folder=None, dataset_directory=None, batch_dice=True, stage=None,\n",
        "                 unpack_data=True, deterministic=True, fp16=False):\n",
        "        \"\"\"\n",
        "        :param deterministic:\n",
        "        :param fold: can be either [0 ... 5) for cross-validation, 'all' to train on all available training data or\n",
        "        None if you wish to load some checkpoint and do inference only\n",
        "        :param plans_file: the pkl file generated by preprocessing. This file will determine all design choices\n",
        "        :param subfolder_with_preprocessed_data: must be a subfolder of dataset_directory (just the name of the folder,\n",
        "        not the entire path). This is where the preprocessed data lies that will be used for network training. We made\n",
        "        this explicitly available so that differently preprocessed data can coexist and the user can choose what to use.\n",
        "        Can be None if you are doing inference only.\n",
        "        :param output_folder: where to store parameters, plot progress and to the validation\n",
        "        :param dataset_directory: the parent directory in which the preprocessed Task data is stored. This is required\n",
        "        because the split information is stored in this directory. For running prediction only this input is not\n",
        "        required and may be set to None\n",
        "        :param batch_dice: compute dice loss for each sample and average over all samples in the batch or pretend the\n",
        "        batch is a pseudo volume?\n",
        "        :param stage: The plans file may contain several stages (used for lowres / highres / pyramid). Stage must be\n",
        "        specified for training:\n",
        "        if stage 1 exists then stage 1 is the high resolution stage, otherwise it's 0\n",
        "        :param unpack_data: if False, npz preprocessed data will not be unpacked to npy. This consumes less space but\n",
        "        is considerably slower! Running unpack_data=False with 2d should never be done!\n",
        "\n",
        "        IMPORTANT: If you inherit from nnUNetTrainer and the init args change then you need to redefine self.init_args\n",
        "        in your init accordingly. Otherwise checkpoints won't load properly!\n",
        "        \"\"\"\n",
        "        super(nnUNetTrainer, self).__init__(deterministic, fp16)\n",
        "        self.unpack_data = unpack_data\n",
        "        self.init_args = (plans_file, fold, output_folder, dataset_directory, batch_dice, stage, unpack_data,\n",
        "                          deterministic, fp16)\n",
        "        # set through arguments from init\n",
        "        self.stage = stage\n",
        "        self.experiment_name = self.__class__.__name__\n",
        "        self.plans_file = plans_file\n",
        "        self.output_folder = output_folder\n",
        "        self.dataset_directory = dataset_directory\n",
        "        self.output_folder_base = self.output_folder\n",
        "        self.fold = fold\n",
        "\n",
        "        self.plans = None\n",
        "\n",
        "        # if we are running inference only then the self.dataset_directory is set (due to checkpoint loading) but it\n",
        "        # irrelevant\n",
        "        if self.dataset_directory is not None and isdir(self.dataset_directory):\n",
        "            self.gt_niftis_folder = join(self.dataset_directory, \"gt_segmentations\")\n",
        "        else:\n",
        "            self.gt_niftis_folder = None\n",
        "\n",
        "        self.folder_with_preprocessed_data = None\n",
        "\n",
        "        # set in self.initialize()\n",
        "\n",
        "        self.dl_tr = self.dl_val = None\n",
        "        self.num_input_channels = self.num_classes = self.net_pool_per_axis = self.patch_size = self.batch_size = \\\n",
        "            self.threeD = self.base_num_features = self.intensity_properties = self.normalization_schemes = \\\n",
        "            self.net_num_pool_op_kernel_sizes = self.net_conv_kernel_sizes = None  # loaded automatically from plans_file\n",
        "        self.basic_generator_patch_size = self.data_aug_params = self.transpose_forward = self.transpose_backward = None\n",
        "\n",
        "        self.batch_dice = batch_dice\n",
        "        self.loss = DC_and_CE_loss({'batch_dice': self.batch_dice, 'smooth': 1e-5, 'do_bg': False}, {})\n",
        "\n",
        "        self.online_eval_foreground_dc = []\n",
        "        self.online_eval_tp = []\n",
        "        self.online_eval_fp = []\n",
        "        self.online_eval_fn = []\n",
        "\n",
        "        self.classes = self.do_dummy_2D_aug = self.use_mask_for_norm = self.only_keep_largest_connected_component = \\\n",
        "            self.min_region_size_per_class = self.min_size_per_class = None\n",
        "\n",
        "        self.inference_pad_border_mode = \"constant\"\n",
        "        self.inference_pad_kwargs = {'constant_values': 0}\n",
        "\n",
        "        self.update_fold(fold)\n",
        "        self.pad_all_sides = None\n",
        "\n",
        "        self.lr_scheduler_eps = 1e-3\n",
        "        self.lr_scheduler_patience = 30\n",
        "        self.initial_lr = 3e-4\n",
        "        self.weight_decay = 3e-5\n",
        "\n",
        "        self.oversample_foreground_percent = 0.33\n",
        "\n",
        "        self.conv_per_stage = None\n",
        "        self.regions_class_order = None\n",
        "\n",
        "    def update_fold(self, fold):\n",
        "        \"\"\"\n",
        "        used to swap between folds for inference (ensemble of models from cross-validation)\n",
        "        DO NOT USE DURING TRAINING AS THIS WILL NOT UPDATE THE DATASET SPLIT AND THE DATA AUGMENTATION GENERATORS\n",
        "        :param fold:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if fold is not None:\n",
        "            if isinstance(fold, str):\n",
        "                assert fold == \"all\", \"if self.fold is a string then it must be \\'all\\'\"\n",
        "                if self.output_folder.endswith(\"%s\" % str(self.fold)):\n",
        "                    self.output_folder = self.output_folder_base\n",
        "                self.output_folder = join(self.output_folder, \"%s\" % str(fold))\n",
        "            else:\n",
        "                if self.output_folder.endswith(\"fold_%s\" % str(self.fold)):\n",
        "                    self.output_folder = self.output_folder_base\n",
        "                self.output_folder = join(self.output_folder, \"fold_%s\" % str(fold))\n",
        "            self.fold = fold\n",
        "\n",
        "    def setup_DA_params(self):\n",
        "        if self.threeD:\n",
        "            self.data_aug_params = default_3D_augmentation_params\n",
        "            if self.do_dummy_2D_aug:\n",
        "                self.data_aug_params[\"dummy_2D\"] = True\n",
        "                self.print_to_log_file(\"Using dummy2d data augmentation\")\n",
        "                self.data_aug_params[\"elastic_deform_alpha\"] = \\\n",
        "                    default_2D_augmentation_params[\"elastic_deform_alpha\"]\n",
        "                self.data_aug_params[\"elastic_deform_sigma\"] = \\\n",
        "                    default_2D_augmentation_params[\"elastic_deform_sigma\"]\n",
        "                self.data_aug_params[\"rotation_x\"] = default_2D_augmentation_params[\"rotation_x\"]\n",
        "        else:\n",
        "            self.do_dummy_2D_aug = False\n",
        "            if max(self.patch_size) / min(self.patch_size) > 1.5:\n",
        "                default_2D_augmentation_params['rotation_x'] = (-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi)\n",
        "            self.data_aug_params = default_2D_augmentation_params\n",
        "        self.data_aug_params[\"mask_was_used_for_normalization\"] = self.use_mask_for_norm\n",
        "\n",
        "        if self.do_dummy_2D_aug:\n",
        "            self.basic_generator_patch_size = get_patch_size(self.patch_size[1:],\n",
        "                                                             self.data_aug_params['rotation_x'],\n",
        "                                                             self.data_aug_params['rotation_y'],\n",
        "                                                             self.data_aug_params['rotation_z'],\n",
        "                                                             self.data_aug_params['scale_range'])\n",
        "            self.basic_generator_patch_size = np.array([self.patch_size[0]] + list(self.basic_generator_patch_size))\n",
        "            patch_size_for_spatialtransform = self.patch_size[1:]\n",
        "        else:\n",
        "            self.basic_generator_patch_size = get_patch_size(self.patch_size, self.data_aug_params['rotation_x'],\n",
        "                                                             self.data_aug_params['rotation_y'],\n",
        "                                                             self.data_aug_params['rotation_z'],\n",
        "                                                             self.data_aug_params['scale_range'])\n",
        "            patch_size_for_spatialtransform = self.patch_size\n",
        "\n",
        "        self.data_aug_params['selected_seg_channels'] = [0]\n",
        "        self.data_aug_params['patch_size_for_spatialtransform'] = patch_size_for_spatialtransform\n",
        "\n",
        "    def initialize(self, training=True, force_load_plans=False):\n",
        "        \"\"\"\n",
        "        For prediction of test cases just set training=False, this will prevent loading of training data and\n",
        "        training batchgenerator initialization\n",
        "        :param training:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        maybe_mkdir_p(self.output_folder)\n",
        "\n",
        "        if force_load_plans or (self.plans is None):\n",
        "            self.load_plans_file()\n",
        "\n",
        "        self.process_plans(self.plans)\n",
        "\n",
        "        self.setup_DA_params()\n",
        "\n",
        "        self.folder_with_preprocessed_data = join(self.dataset_directory, self.plans['data_identifier'] +\n",
        "                                                  \"_stage%d\" % self.stage)\n",
        "        if training:\n",
        "            self.dl_tr, self.dl_val = self.get_basic_generators()\n",
        "            if self.unpack_data:\n",
        "                self.print_to_log_file(\"unpacking dataset\")\n",
        "                unpack_dataset(self.folder_with_preprocessed_data)\n",
        "                self.print_to_log_file(\"done\")\n",
        "            else:\n",
        "                self.print_to_log_file(\n",
        "                    \"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"\n",
        "                    \"will wait all winter for your model to finish!\")\n",
        "            self.tr_gen, self.val_gen = get_default_augmentation(self.dl_tr, self.dl_val,\n",
        "                                                                 self.data_aug_params[\n",
        "                                                                     'patch_size_for_spatialtransform'],\n",
        "                                                                 self.data_aug_params)\n",
        "            self.print_to_log_file(\"TRAINING KEYS:\\n %s\" % (str(self.dataset_tr.keys())),\n",
        "                                   also_print_to_console=False)\n",
        "            self.print_to_log_file(\"VALIDATION KEYS:\\n %s\" % (str(self.dataset_val.keys())),\n",
        "                                   also_print_to_console=False)\n",
        "        else:\n",
        "            pass\n",
        "        self.initialize_network()\n",
        "        self.initialize_optimizer_and_scheduler()\n",
        "        # assert isinstance(self.network, (SegmentationNetwork, nn.DataParallel))\n",
        "        self.was_initialized = True\n",
        "\n",
        "    def initialize_network(self):\n",
        "        \"\"\"\n",
        "        This is specific to the U-Net and must be adapted for other network architectures\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # self.print_to_log_file(self.net_num_pool_op_kernel_sizes)\n",
        "        # self.print_to_log_file(self.net_conv_kernel_sizes)\n",
        "\n",
        "        net_numpool = len(self.net_num_pool_op_kernel_sizes)\n",
        "\n",
        "        if self.threeD:\n",
        "            conv_op = nn.Conv3d\n",
        "            dropout_op = nn.Dropout3d\n",
        "            norm_op = nn.InstanceNorm3d\n",
        "        else:\n",
        "            conv_op = nn.Conv2d\n",
        "            dropout_op = nn.Dropout2d\n",
        "            norm_op = nn.InstanceNorm2d\n",
        "\n",
        "        norm_op_kwargs = {'eps': 1e-5, 'affine': True}\n",
        "        dropout_op_kwargs = {'p': 0, 'inplace': True}\n",
        "        net_nonlin = nn.LeakyReLU\n",
        "        net_nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
        "        self.network = Generic_UNet(self.num_input_channels, self.base_num_features, self.num_classes, net_numpool,\n",
        "                                    self.conv_per_stage, 2, conv_op, norm_op, norm_op_kwargs, dropout_op, dropout_op_kwargs,\n",
        "                                    net_nonlin, net_nonlin_kwargs, False, False, lambda x: x, InitWeights_He(1e-2),\n",
        "                                    self.net_num_pool_op_kernel_sizes, self.net_conv_kernel_sizes, False, True, True)\n",
        "        # self.network.inference_apply_nonlin = softmax_helper\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.network.cuda()\n",
        "\n",
        "    def initialize_optimizer_and_scheduler(self):\n",
        "        assert self.network is not None, \"self.initialize_network must be called first\"\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay,\n",
        "                                          amsgrad=True)\n",
        "        self.lr_scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.2,\n",
        "                                                           patience=self.lr_scheduler_patience,\n",
        "                                                           verbose=True, threshold=self.lr_scheduler_eps,\n",
        "                                                           threshold_mode=\"abs\")\n",
        "\n",
        "    def plot_network_architecture(self):\n",
        "        try:\n",
        "            from batchgenerators.utilities.file_and_folder_operations import join\n",
        "            import hiddenlayer as hl\n",
        "            if torch.cuda.is_available():\n",
        "                g = hl.build_graph(self.network, torch.rand((1, self.num_input_channels, *self.patch_size)).cuda(),\n",
        "                                   transforms=None)\n",
        "            else:\n",
        "                g = hl.build_graph(self.network, torch.rand((1, self.num_input_channels, *self.patch_size)),\n",
        "                                   transforms=None)\n",
        "            g.save(join(self.output_folder, \"network_architecture.pdf\"))\n",
        "            del g\n",
        "        except Exception as e:\n",
        "            self.print_to_log_file(\"Unable to plot network architecture:\")\n",
        "            self.print_to_log_file(e)\n",
        "\n",
        "            self.print_to_log_file(\"\\nprinting the network instead:\\n\")\n",
        "            self.print_to_log_file(self.network)\n",
        "            self.print_to_log_file(\"\\n\")\n",
        "        finally:\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    def run_training(self):\n",
        "        dct = OrderedDict()\n",
        "        for k in self.__dir__():\n",
        "            if not k.startswith(\"__\"):\n",
        "                if not callable(getattr(self, k)):\n",
        "                    dct[k] = str(getattr(self, k))\n",
        "        del dct['plans']\n",
        "        del dct['intensity_properties']\n",
        "        del dct['dataset']\n",
        "        del dct['dataset_tr']\n",
        "        del dct['dataset_val']\n",
        "        save_json(dct, join(self.output_folder, \"debug.json\"))\n",
        "\n",
        "        import shutil\n",
        "\n",
        "        shutil.copy(self.plans_file, join(self.output_folder_base, \"plans.pkl\"))\n",
        "\n",
        "        super(nnUNetTrainer, self).run_training()\n",
        "\n",
        "    def load_plans_file(self):\n",
        "        \"\"\"\n",
        "        This is what actually configures the entire experiment. The plans file is generated by experiment planning\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.plans = load_pickle(self.plans_file)\n",
        "\n",
        "    def process_plans(self, plans):\n",
        "        if self.stage is None:\n",
        "            assert len(list(plans['plans_per_stage'].keys())) == 1, \\\n",
        "                \"If self.stage is None then there can be only one stage in the plans file. That seems to not be the \" \\\n",
        "                \"case. Please specify which stage of the cascade must be trained\"\n",
        "            self.stage = list(plans['plans_per_stage'].keys())[0]\n",
        "        self.plans = plans\n",
        "\n",
        "        stage_plans = self.plans['plans_per_stage'][self.stage]\n",
        "        self.batch_size = stage_plans['batch_size']\n",
        "        self.net_pool_per_axis = stage_plans['num_pool_per_axis']\n",
        "        self.patch_size = np.array(stage_plans['patch_size']).astype(int)\n",
        "        self.do_dummy_2D_aug = stage_plans['do_dummy_2D_data_aug']\n",
        "        self.net_num_pool_op_kernel_sizes = stage_plans['pool_op_kernel_sizes']\n",
        "        self.net_conv_kernel_sizes = stage_plans['conv_kernel_sizes']\n",
        "\n",
        "        self.pad_all_sides = None  # self.patch_size\n",
        "        self.intensity_properties = plans['dataset_properties']['intensityproperties']\n",
        "        self.normalization_schemes = plans['normalization_schemes']\n",
        "        self.base_num_features = plans['base_num_features']\n",
        "        self.num_input_channels = plans['num_modalities']\n",
        "        self.num_classes = plans['num_classes'] + 1  # background is no longer in num_classes\n",
        "        self.classes = plans['all_classes']\n",
        "        self.use_mask_for_norm = plans['use_mask_for_norm']\n",
        "        self.only_keep_largest_connected_component = plans['keep_only_largest_region']\n",
        "        self.min_region_size_per_class = plans['min_region_size_per_class']\n",
        "        self.min_size_per_class = None  # DONT USE THIS. plans['min_size_per_class']\n",
        "\n",
        "        if plans.get('transpose_forward') is None or plans.get('transpose_backward') is None:\n",
        "            print(\"WARNING! You seem to have data that was preprocessed with a previous version of nnU-Net. \"\n",
        "                  \"You should rerun preprocessing. We will proceed and assume that both transpose_foward \"\n",
        "                  \"and transpose_backward are [0, 1, 2]. If that is not correct then weird things will happen!\")\n",
        "            plans['transpose_forward'] = [0, 1, 2]\n",
        "            plans['transpose_backward'] = [0, 1, 2]\n",
        "        self.transpose_forward = plans['transpose_forward']\n",
        "        self.transpose_backward = plans['transpose_backward']\n",
        "\n",
        "        if len(self.patch_size) == 2:\n",
        "            self.threeD = False\n",
        "        elif len(self.patch_size) == 3:\n",
        "            self.threeD = True\n",
        "        else:\n",
        "            raise RuntimeError(\"invalid patch size in plans file: %s\" % str(self.patch_size))\n",
        "\n",
        "        if \"conv_per_stage\" in plans.keys():  # this ha sbeen added to the plans only recently\n",
        "            self.conv_per_stage = plans['conv_per_stage']\n",
        "        else:\n",
        "            self.conv_per_stage = 2\n",
        "\n",
        "    def load_dataset(self):\n",
        "        self.dataset = load_dataset(self.folder_with_preprocessed_data)\n",
        "\n",
        "    def get_basic_generators(self):\n",
        "        self.load_dataset()\n",
        "        self.do_split()\n",
        "\n",
        "        if self.threeD:\n",
        "            dl_tr = DataLoader3D(self.dataset_tr, self.basic_generator_patch_size, self.patch_size, self.batch_size,\n",
        "                                 False, oversample_foreground_percent=self.oversample_foreground_percent,\n",
        "                                 pad_mode=\"constant\", pad_sides=self.pad_all_sides)\n",
        "            dl_val = DataLoader3D(self.dataset_val, self.patch_size, self.patch_size, self.batch_size, False,\n",
        "                                  oversample_foreground_percent=self.oversample_foreground_percent,\n",
        "                                  pad_mode=\"constant\", pad_sides=self.pad_all_sides)\n",
        "        else:\n",
        "            dl_tr = DataLoader2D(self.dataset_tr, self.basic_generator_patch_size, self.patch_size, self.batch_size,\n",
        "                                 transpose=None,  # self.plans.get('transpose_forward'),\n",
        "                                 oversample_foreground_percent=self.oversample_foreground_percent,\n",
        "                                 pad_mode=\"constant\", pad_sides=self.pad_all_sides)\n",
        "            dl_val = DataLoader2D(self.dataset_val, self.patch_size, self.patch_size, self.batch_size,\n",
        "                                  transpose=None,  # self.plans.get('transpose_forward'),\n",
        "                                  oversample_foreground_percent=self.oversample_foreground_percent,\n",
        "                                  pad_mode=\"constant\", pad_sides=self.pad_all_sides)\n",
        "        return dl_tr, dl_val\n",
        "\n",
        "    def preprocess_patient(self, input_files):\n",
        "        \"\"\"\n",
        "        Used to predict new unseen data. Not used for the preprocessing of the training/test data\n",
        "        :param input_files:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        from nnunet.training.model_restore import recursive_find_python_class\n",
        "        preprocessor_name = self.plans.get('preprocessor_name')\n",
        "        if preprocessor_name is None:\n",
        "            if self.threeD:\n",
        "                preprocessor_name = \"GenericPreprocessor\"\n",
        "            else:\n",
        "                preprocessor_name = \"PreprocessorFor2D\"\n",
        "\n",
        "        print(\"using preprocessor\", preprocessor_name)\n",
        "        preprocessor_class = recursive_find_python_class([join(nnunet.__path__[0], \"preprocessing\")],\n",
        "                                                         preprocessor_name,\n",
        "                                                         current_module=\"nnunet.preprocessing\")\n",
        "        assert preprocessor_class is not None, \"Could not find preprocessor %s in nnunet.preprocessing\" % \\\n",
        "                                               preprocessor_name\n",
        "        preprocessor = preprocessor_class(self.normalization_schemes, self.use_mask_for_norm,\n",
        "                                           self.transpose_forward, self.intensity_properties)\n",
        "\n",
        "        d, s, properties = preprocessor.preprocess_test_case(input_files,\n",
        "                                                             self.plans['plans_per_stage'][self.stage][\n",
        "                                                                 'current_spacing'])\n",
        "        return d, s, properties\n",
        "\n",
        "    def preprocess_predict_nifti(self, input_files: List[str], output_file: str = None,\n",
        "                                 softmax_ouput_file: str = None) -> None:\n",
        "        \"\"\"\n",
        "        Use this to predict new data\n",
        "        :param input_files:\n",
        "        :param output_file:\n",
        "        :param softmax_ouput_file:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        print(\"preprocessing...\")\n",
        "        d, s, properties = self.preprocess_patient(input_files)\n",
        "        print(\"predicting...\")\n",
        "        pred = self.predict_preprocessed_data_return_seg_and_softmax(d, self.data_aug_params[\"do_mirror\"],\n",
        "                                                                     self.data_aug_params['mirror_axes'], True, 0.5,\n",
        "                                                                     True, 'constant', {'constant_values': 0},\n",
        "                                                                     self.patch_size, True)[1]\n",
        "        pred = pred.transpose([0] + [i + 1 for i in self.transpose_backward])\n",
        "\n",
        "        print(\"resampling to original spacing and nifti export...\")\n",
        "        save_segmentation_nifti_from_softmax(pred, output_file, properties, 3, None, None, None, softmax_ouput_file,\n",
        "                                             None)\n",
        "        print(\"done\")\n",
        "\n",
        "    def predict_preprocessed_data_return_seg_and_softmax(self, data: np.ndarray, do_mirroring: bool = True,\n",
        "                                                         mirror_axes: Tuple[int] = None, use_sliding_window: bool = True,\n",
        "                                                         step_size: float = 0.5, use_gaussian: bool = True,\n",
        "                                                         pad_border_mode: str = 'constant', pad_kwargs: dict = None,\n",
        "                                                         all_in_gpu: bool = True,\n",
        "                                                         verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        :param data:\n",
        "        :param do_mirroring:\n",
        "        :param mirror_axes:\n",
        "        :param use_sliding_window:\n",
        "        :param step_size:\n",
        "        :param use_gaussian:\n",
        "        :param pad_border_mode:\n",
        "        :param pad_kwargs:\n",
        "        :param all_in_gpu:\n",
        "        :param verbose:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if pad_border_mode == 'constant' and pad_kwargs is None:\n",
        "            pad_kwargs = {'constant_values': 0}\n",
        "\n",
        "        if do_mirroring and mirror_axes is None:\n",
        "            mirror_axes = self.data_aug_params['mirror_axes']\n",
        "\n",
        "        if do_mirroring:\n",
        "            assert self.data_aug_params[\"do_mirror\"], \"Cannot do mirroring as test time augmentation when training \" \\\n",
        "                                                      \"was done without mirroring\"\n",
        "\n",
        "        valid = list((SegmentationNetwork, nn.DataParallel))\n",
        "        assert isinstance(self.network, tuple(valid))\n",
        "\n",
        "        current_mode = self.network.training\n",
        "        self.network.eval()\n",
        "        ret = self.network.predict_3D(data, do_mirroring, mirror_axes, use_sliding_window, step_size, self.patch_size,\n",
        "                                       self.regions_class_order, use_gaussian, pad_border_mode, pad_kwargs,\n",
        "                                       all_in_gpu, verbose)\n",
        "        self.network.train(current_mode)\n",
        "        return ret\n",
        "\n",
        "    def validate(self, do_mirroring: bool = True, use_sliding_window: bool = True, step_size: float = 0.5,\n",
        "                 save_softmax: bool = True, use_gaussian: bool = True, overwrite: bool = True,\n",
        "                 validation_folder_name: str = 'validation_raw', debug: bool = False, all_in_gpu: bool = False,\n",
        "                 force_separate_z: bool = None, interpolation_order: int = 3, interpolation_order_z: int = 0):\n",
        "        \"\"\"\n",
        "        if debug=True then the temporary files generated for postprocessing determination will be kept\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        current_mode = self.network.training\n",
        "        self.network.eval()\n",
        "\n",
        "        assert self.was_initialized, \"must initialize, ideally with checkpoint (or train first)\"\n",
        "        if self.dataset_val is None:\n",
        "            self.load_dataset()\n",
        "            self.do_split()\n",
        "\n",
        "        # predictions as they come from the network go here\n",
        "        output_folder = join(self.output_folder, validation_folder_name)\n",
        "        maybe_mkdir_p(output_folder)\n",
        "        # this is for debug purposes\n",
        "        my_input_args = {'do_mirroring': do_mirroring,\n",
        "                         'use_sliding_window': use_sliding_window,\n",
        "                         'step_size': step_size,\n",
        "                         'save_softmax': save_softmax,\n",
        "                         'use_gaussian': use_gaussian,\n",
        "                         'overwrite': overwrite,\n",
        "                         'validation_folder_name': validation_folder_name,\n",
        "                         'debug': debug,\n",
        "                         'all_in_gpu': all_in_gpu,\n",
        "                         'force_separate_z': force_separate_z,\n",
        "                         'interpolation_order': interpolation_order,\n",
        "                         'interpolation_order_z': interpolation_order_z,\n",
        "                         }\n",
        "        save_json(my_input_args, join(output_folder, \"validation_args.json\"))\n",
        "\n",
        "        if do_mirroring:\n",
        "            if not self.data_aug_params['do_mirror']:\n",
        "                raise RuntimeError(\"We did not train with mirroring so you cannot do inference with mirroring enabled\")\n",
        "            mirror_axes = self.data_aug_params['mirror_axes']\n",
        "        else:\n",
        "            mirror_axes = ()\n",
        "\n",
        "        pred_gt_tuples = []\n",
        "\n",
        "        export_pool = Pool(default_num_threads)\n",
        "        results = []\n",
        "\n",
        "        for k in self.dataset_val.keys():\n",
        "            properties = self.dataset[k]['properties']\n",
        "            fname = properties['list_of_data_files'][0].split(\"/\")[-1][:-12]\n",
        "            if overwrite or (not isfile(join(output_folder, fname + \".nii.gz\"))) or \\\n",
        "                    (save_softmax and not isfile(join(output_folder, fname + \".npz\"))):\n",
        "                data = np.load(self.dataset[k]['data_file'])['data']\n",
        "\n",
        "                print(k, data.shape)\n",
        "                data[-1][data[-1] == -1] = 0\n",
        "\n",
        "                softmax_pred = self.predict_preprocessed_data_return_seg_and_softmax(\n",
        "                    data[:-1], do_mirroring, mirror_axes, use_sliding_window, step_size, use_gaussian, all_in_gpu=all_in_gpu\n",
        "                )[1]\n",
        "\n",
        "                softmax_pred = softmax_pred.transpose([0] + [i + 1 for i in self.transpose_backward])\n",
        "\n",
        "                if save_softmax:\n",
        "                    softmax_fname = join(output_folder, fname + \".npz\")\n",
        "                else:\n",
        "                    softmax_fname = None\n",
        "\n",
        "                \"\"\"There is a problem with python process communication that prevents us from communicating obejcts\n",
        "                larger than 2 GB between processes (basically when the length of the pickle string that will be sent is\n",
        "                communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long\n",
        "                enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually\n",
        "                patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will\n",
        "                then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either\n",
        "                filename or np.ndarray and will handle this automatically\"\"\"\n",
        "                if np.prod(softmax_pred.shape) > (2e9 / 4 * 0.85):  # *0.85 just to be save\n",
        "                    np.save(join(output_folder, fname + \".npy\"), softmax_pred)\n",
        "                    softmax_pred = join(output_folder, fname + \".npy\")\n",
        "\n",
        "                results.append(export_pool.starmap_async(save_segmentation_nifti_from_softmax,\n",
        "                                                         ((softmax_pred, join(output_folder, fname + \".nii.gz\"),\n",
        "                                                           properties, interpolation_order, None, None, None,\n",
        "                                                           softmax_fname, None, force_separate_z,\n",
        "                                                           interpolation_order_z),\n",
        "                                                          )\n",
        "                                                         )\n",
        "                               )\n",
        "\n",
        "            pred_gt_tuples.append([join(output_folder, fname + \".nii.gz\"),\n",
        "                                   join(self.gt_niftis_folder, fname + \".nii.gz\")])\n",
        "\n",
        "        _ = [i.get() for i in results]\n",
        "        self.print_to_log_file(\"finished prediction\")\n",
        "\n",
        "        # evaluate raw predictions\n",
        "        self.print_to_log_file(\"evaluation of raw predictions\")\n",
        "        task = self.dataset_directory.split(\"/\")[-1]\n",
        "        job_name = self.experiment_name\n",
        "        _ = aggregate_scores(pred_gt_tuples, labels=list(range(self.num_classes)),\n",
        "                             json_output_file=join(output_folder, \"summary.json\"),\n",
        "                             json_name=job_name + \" val tiled %s\" % (str(use_sliding_window)),\n",
        "                             json_author=\"Fabian\",\n",
        "                             json_task=task, num_threads=default_num_threads)\n",
        "\n",
        "        # in the old nnunet we would stop here. Now we add a postprocessing. This postprocessing can remove everything\n",
        "        # except the largest connected component for each class. To see if this improves results, we do this for all\n",
        "        # classes and then rerun the evaluation. Those classes for which this resulted in an improved dice score will\n",
        "        # have this applied during inference as well\n",
        "        self.print_to_log_file(\"determining postprocessing\")\n",
        "        determine_postprocessing(self.output_folder, self.gt_niftis_folder, validation_folder_name,\n",
        "                                 final_subf_name=validation_folder_name + \"_postprocessed\", debug=debug)\n",
        "        # after this the final predictions for the vlaidation set can be found in validation_folder_name_base + \"_postprocessed\"\n",
        "        # They are always in that folder, even if no postprocessing as applied!\n",
        "\n",
        "        # detemining postprocesing on a per-fold basis may be OK for this fold but what if another fold finds another\n",
        "        # postprocesing to be better? In this case we need to consolidate. At the time the consolidation is going to be\n",
        "        # done we won't know what self.gt_niftis_folder was, so now we copy all the niftis into a separate folder to\n",
        "        # be used later\n",
        "        gt_nifti_folder = join(self.output_folder_base, \"gt_niftis\")\n",
        "        maybe_mkdir_p(gt_nifti_folder)\n",
        "        for f in subfiles(self.gt_niftis_folder, suffix=\".nii.gz\"):\n",
        "            success = False\n",
        "            attempts = 0\n",
        "            e = None\n",
        "            while not success and attempts < 10:\n",
        "                try:\n",
        "                    shutil.copy(f, gt_nifti_folder)\n",
        "                    success = True\n",
        "                except OSError as e:\n",
        "                    attempts += 1\n",
        "                    sleep(1)\n",
        "            if not success:\n",
        "                print(\"Could not copy gt nifti file %s into folder %s\" % (f, gt_nifti_folder))\n",
        "                if e is not None:\n",
        "                    raise e\n",
        "\n",
        "        self.network.train(current_mode)\n",
        "\n",
        "    def run_online_evaluation(self, output, target):\n",
        "        with torch.no_grad():\n",
        "            num_classes = output.shape[1]\n",
        "            output_softmax = softmax_helper(output)\n",
        "            output_seg = output_softmax.argmax(1)\n",
        "            target = target[:, 0]\n",
        "            axes = tuple(range(1, len(target.shape)))\n",
        "            tp_hard = torch.zeros((target.shape[0], num_classes - 1)).to(output_seg.device.index)\n",
        "            fp_hard = torch.zeros((target.shape[0], num_classes - 1)).to(output_seg.device.index)\n",
        "            fn_hard = torch.zeros((target.shape[0], num_classes - 1)).to(output_seg.device.index)\n",
        "            for c in range(1, num_classes):\n",
        "                tp_hard[:, c - 1] = sum_tensor((output_seg == c).float() * (target == c).float(), axes=axes)\n",
        "                fp_hard[:, c - 1] = sum_tensor((output_seg == c).float() * (target != c).float(), axes=axes)\n",
        "                fn_hard[:, c - 1] = sum_tensor((output_seg != c).float() * (target == c).float(), axes=axes)\n",
        "\n",
        "            tp_hard = tp_hard.sum(0, keepdim=False).detach().cpu().numpy()\n",
        "            fp_hard = fp_hard.sum(0, keepdim=False).detach().cpu().numpy()\n",
        "            fn_hard = fn_hard.sum(0, keepdim=False).detach().cpu().numpy()\n",
        "\n",
        "            self.online_eval_foreground_dc.append(list((2 * tp_hard) / (2 * tp_hard + fp_hard + fn_hard + 1e-8)))\n",
        "            self.online_eval_tp.append(list(tp_hard))\n",
        "            self.online_eval_fp.append(list(fp_hard))\n",
        "            self.online_eval_fn.append(list(fn_hard))\n",
        "\n",
        "    def finish_online_evaluation(self):\n",
        "        self.online_eval_tp = np.sum(self.online_eval_tp, 0)\n",
        "        self.online_eval_fp = np.sum(self.online_eval_fp, 0)\n",
        "        self.online_eval_fn = np.sum(self.online_eval_fn, 0)\n",
        "\n",
        "        global_dc_per_class = [i for i in [2 * i / (2 * i + j + k) for i, j, k in\n",
        "                                           zip(self.online_eval_tp, self.online_eval_fp, self.online_eval_fn)]\n",
        "                               if not np.isnan(i)]\n",
        "        self.all_val_eval_metrics.append(np.mean(global_dc_per_class))\n",
        "\n",
        "        self.print_to_log_file(\"Average global foreground Dice:\", str(global_dc_per_class))\n",
        "        self.print_to_log_file(\"(interpret this as an estimate for the Dice of the different classes. This is not \"\n",
        "                               \"exact.)\")\n",
        "\n",
        "        self.online_eval_foreground_dc = []\n",
        "        self.online_eval_tp = []\n",
        "        self.online_eval_fp = []\n",
        "        self.online_eval_fn = []\n",
        "\n",
        "    def save_checkpoint(self, fname, save_optimizer=True):\n",
        "        super(nnUNetTrainer, self).save_checkpoint(fname, save_optimizer)\n",
        "        info = OrderedDict()\n",
        "        info['init'] = self.init_args\n",
        "        info['name'] = self.__class__.__name__\n",
        "        info['class'] = str(self.__class__)\n",
        "        info['plans'] = self.plans\n",
        "\n",
        "        write_pickle(info, fname + \".pkl\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/nnUNet/nnunet/training/network_training/nnUNetTrainer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ylZTrAuiCg",
        "colab_type": "text"
      },
      "source": [
        "The reconstruction process from the patch-segmentations back to the complete segmentation stretches across multiple classes in the nnUNet. The actual class discrimination in the nnUNet is done after the reconstruction is completed in the class segmentation_export.py in `/content/nnUNet/nnunet/inference`. But because attention maps generated by Gcam are class discriminant by default (except for Guided Backpropagation) the nnUNet class discriminations needs to be disabled. This is done by commenting out the following block in segmentation_export.py and can be done by executing the next cell:\n",
        "\n",
        "```\n",
        "# Line 114-158\n",
        "seg_old_spacing_org = seg_old_spacing\n",
        "output_filename_org = out_fname\n",
        "for i in range(3):\n",
        "    #seg_old_spacing = np.expand_dims(seg_old_spacing_org[i]*255, axis=0)\n",
        "    seg_old_spacing = seg_old_spacing_org[i] * 255\n",
        "    out_fname = output_filename_org[:-7] + \"_ \"+ str(i) + \".nii.gz\"\n",
        "\n",
        "    #if region_class_order is None:\n",
        "    #    seg_old_spacing = seg_old_spacing.argmax(0)\n",
        "    #else:\n",
        "    #    seg_old_spacing_final = np.zeros(seg_old_spacing.shape[1:])\n",
        "    #    for i, c in enumerate(region_class_order):\n",
        "    #        seg_old_spacing_final[seg_old_spacing[i] > 0.5] = c\n",
        "    #    seg_old_spacing = seg_old_spacing_final\n",
        "\n",
        "    # Original nnUNet code ...\n",
        "    # ...\n",
        "    # ...\n",
        "\n",
        "return output_filename_org\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRKUNRpig7bR",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "f7baa7d4-9735-4d2c-eb33-048d3b008499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Modified segmentation_export.py - Replaces original segmentation_export.py\n",
        "%%writefile /content/nnUNet/nnunet/inference/segmentation_export.py\n",
        "\n",
        "#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany\n",
        "#\n",
        "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#    you may not use this file except in compliance with the License.\n",
        "#    You may obtain a copy of the License at\n",
        "#\n",
        "#        http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#    Unless required by applicable law or agreed to in writing, software\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#    See the License for the specific language governing permissions and\n",
        "#    limitations under the License.\n",
        "\n",
        "\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "from typing import Union, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from batchgenerators.augmentations.utils import resize_segmentation\n",
        "from nnunet.preprocessing.preprocessing import get_lowres_axis, get_do_separate_z, resample_data_or_seg\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "\n",
        "\n",
        "def save_segmentation_nifti_from_softmax(segmentation_softmax: Union[str, np.ndarray], out_fname: str,\n",
        "                                         properties_dict: dict, order: int = 1,\n",
        "                                         region_class_order: Tuple[Tuple[int]] = None,\n",
        "                                         seg_postprogess_fn: callable = None, seg_postprocess_args: tuple = None,\n",
        "                                         resampled_npz_fname: str = None,\n",
        "                                         non_postprocessed_fname: str = None, force_separate_z: bool = None,\n",
        "                                         interpolation_order_z: int = 0, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    This is a utility for writing segmentations to nifto and npz. It requires the data to have been preprocessed by\n",
        "    GenericPreprocessor because it depends on the property dictionary output (dct) to know the geometry of the original\n",
        "    data. segmentation_softmax does not have to have the same size in pixels as the original data, it will be\n",
        "    resampled to match that. This is generally useful because the spacings our networks operate on are most of the time\n",
        "    not the native spacings of the image data.\n",
        "    If seg_postprogess_fn is not None then seg_postprogess_fnseg_postprogess_fn(segmentation, *seg_postprocess_args)\n",
        "    will be called before nifto export\n",
        "    There is a problem with python process communication that prevents us from communicating obejcts\n",
        "    larger than 2 GB between processes (basically when the length of the pickle string that will be sent is\n",
        "    communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long\n",
        "    enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually\n",
        "    patching system python code.) We circumvent that problem here by saving softmax_pred to a npy file that will\n",
        "    then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either\n",
        "    filename or np.ndarray for segmentation_softmax and will handle this automatically\n",
        "    :param segmentation_softmax:\n",
        "    :param out_fname:\n",
        "    :param properties_dict:\n",
        "    :param order:\n",
        "    :param region_class_order:\n",
        "    :param seg_postprogess_fn:\n",
        "    :param seg_postprocess_args:\n",
        "    :param resampled_npz_fname:\n",
        "    :param non_postprocessed_fname:\n",
        "    :param force_separate_z: if None then we dynamically decide how to resample along z, if True/False then always\n",
        "    /never resample along z separately. Do not touch unless you know what you are doing\n",
        "    :param interpolation_order_z: if separate z resampling is done then this is the order for resampling in z\n",
        "    :param verbose:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if verbose: print(\"force_separate_z:\", force_separate_z, \"interpolation order:\", order)\n",
        "\n",
        "    if isinstance(segmentation_softmax, str):\n",
        "        assert isfile(segmentation_softmax), \"If isinstance(segmentation_softmax, str) then \" \\\n",
        "                                             \"isfile(segmentation_softmax) must be True\"\n",
        "        del_file = deepcopy(segmentation_softmax)\n",
        "        segmentation_softmax = np.load(segmentation_softmax)\n",
        "        os.remove(del_file)\n",
        "\n",
        "    # first resample, then put result into bbox of cropping, then save\n",
        "    current_shape = segmentation_softmax.shape\n",
        "    shape_original_after_cropping = properties_dict.get('size_after_cropping')\n",
        "    shape_original_before_cropping = properties_dict.get('original_size_of_raw_data')\n",
        "    # current_spacing = dct.get('spacing_after_resampling')\n",
        "    # original_spacing = dct.get('original_spacing')\n",
        "\n",
        "    if np.any([i != j for i, j in zip(np.array(current_shape[1:]), np.array(shape_original_after_cropping))]):\n",
        "        if force_separate_z is None:\n",
        "            if get_do_separate_z(properties_dict.get('original_spacing')):\n",
        "                do_separate_z = True\n",
        "                lowres_axis = get_lowres_axis(properties_dict.get('original_spacing'))\n",
        "            elif get_do_separate_z(properties_dict.get('spacing_after_resampling')):\n",
        "                do_separate_z = True\n",
        "                lowres_axis = get_lowres_axis(properties_dict.get('spacing_after_resampling'))\n",
        "            else:\n",
        "                do_separate_z = False\n",
        "                lowres_axis = None\n",
        "        else:\n",
        "            do_separate_z = force_separate_z\n",
        "            if do_separate_z:\n",
        "                lowres_axis = get_lowres_axis(properties_dict.get('original_spacing'))\n",
        "            else:\n",
        "                lowres_axis = None\n",
        "\n",
        "        if verbose: print(\"separate z:\", do_separate_z, \"lowres axis\", lowres_axis)\n",
        "        seg_old_spacing = resample_data_or_seg(segmentation_softmax, shape_original_after_cropping, is_seg=False,\n",
        "                                               axis=lowres_axis, order=order, do_separate_z=do_separate_z, cval=0,\n",
        "                                               order_z=interpolation_order_z)\n",
        "        # seg_old_spacing = resize_softmax_output(segmentation_softmax, shape_original_after_cropping, order=order)\n",
        "    else:\n",
        "        if verbose: print(\"no resampling necessary\")\n",
        "        seg_old_spacing = segmentation_softmax\n",
        "\n",
        "    if resampled_npz_fname is not None:\n",
        "        np.savez_compressed(resampled_npz_fname, softmax=seg_old_spacing.astype(np.float16))\n",
        "        save_pickle(properties_dict, resampled_npz_fname[:-4] + \".pkl\")\n",
        "\n",
        "    seg_old_spacing_org = seg_old_spacing\n",
        "    output_filename_org = out_fname\n",
        "    for i in range(3):\n",
        "        seg_old_spacing = seg_old_spacing_org[i] * 255\n",
        "        out_fname = output_filename_org[:-7] + \"_ \"+ str(i) + \".nii.gz\"\n",
        "\n",
        "        # if region_class_order is None:\n",
        "        #     seg_old_spacing = seg_old_spacing.argmax(0)\n",
        "        # else:\n",
        "        #     seg_old_spacing_final = np.zeros(seg_old_spacing.shape[1:])\n",
        "        #     for i, c in enumerate(region_class_order):\n",
        "        #         seg_old_spacing_final[seg_old_spacing[i] > 0.5] = c\n",
        "        #     seg_old_spacing = seg_old_spacing_final\n",
        "\n",
        "        bbox = properties_dict.get('crop_bbox')\n",
        "\n",
        "        if bbox is not None:\n",
        "            seg_old_size = np.zeros(shape_original_before_cropping)\n",
        "            for c in range(3):\n",
        "                bbox[c][1] = np.min((bbox[c][0] + seg_old_spacing.shape[c], shape_original_before_cropping[c]))\n",
        "            seg_old_size[bbox[0][0]:bbox[0][1],\n",
        "            bbox[1][0]:bbox[1][1],\n",
        "            bbox[2][0]:bbox[2][1]] = seg_old_spacing\n",
        "        else:\n",
        "            seg_old_size = seg_old_spacing\n",
        "\n",
        "        if seg_postprogess_fn is not None:\n",
        "            seg_old_size_postprocessed = seg_postprogess_fn(np.copy(seg_old_size), *seg_postprocess_args)\n",
        "        else:\n",
        "            seg_old_size_postprocessed = seg_old_size\n",
        "\n",
        "        seg_resized_itk = sitk.GetImageFromArray(seg_old_size_postprocessed.astype(np.uint8))\n",
        "        seg_resized_itk.SetSpacing(properties_dict['itk_spacing'])\n",
        "        seg_resized_itk.SetOrigin(properties_dict['itk_origin'])\n",
        "        seg_resized_itk.SetDirection(properties_dict['itk_direction'])\n",
        "        sitk.WriteImage(seg_resized_itk, out_fname)\n",
        "\n",
        "        if (non_postprocessed_fname is not None) and (seg_postprogess_fn is not None):\n",
        "            seg_resized_itk = sitk.GetImageFromArray(seg_old_size.astype(np.uint8))\n",
        "            seg_resized_itk.SetSpacing(properties_dict['itk_spacing'])\n",
        "            seg_resized_itk.SetOrigin(properties_dict['itk_origin'])\n",
        "            seg_resized_itk.SetDirection(properties_dict['itk_direction'])\n",
        "            sitk.WriteImage(seg_resized_itk, non_postprocessed_fname)\n",
        "          \n",
        "    return output_filename_org\n",
        "\n",
        "\n",
        "def save_segmentation_nifti(segmentation, out_fname, dct, order=1, force_separate_z=None):\n",
        "    \"\"\"\n",
        "    faster and uses less ram than save_segmentation_nifti_from_softmax, but maybe less precise and also does not support\n",
        "    softmax export (which is needed for ensembling). So it's a niche function that may be useful in some cases.\n",
        "    :param segmentation:\n",
        "    :param out_fname:\n",
        "    :param dct:\n",
        "    :param order:\n",
        "    :param force_separate_z:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # suppress output\n",
        "    print(\"force_separate_z:\", force_separate_z, \"interpolation order:\", order)\n",
        "    sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    if isinstance(segmentation, str):\n",
        "        assert isfile(segmentation), \"If isinstance(segmentation_softmax, str) then \" \\\n",
        "                                     \"isfile(segmentation_softmax) must be True\"\n",
        "        del_file = deepcopy(segmentation)\n",
        "        segmentation = np.load(segmentation)\n",
        "        os.remove(del_file)\n",
        "\n",
        "    # first resample, then put result into bbox of cropping, then save\n",
        "    current_shape = segmentation.shape\n",
        "    shape_original_after_cropping = dct.get('size_after_cropping')\n",
        "    shape_original_before_cropping = dct.get('original_size_of_raw_data')\n",
        "    # current_spacing = dct.get('spacing_after_resampling')\n",
        "    # original_spacing = dct.get('original_spacing')\n",
        "\n",
        "    if np.any(np.array(current_shape) != np.array(shape_original_after_cropping)):\n",
        "        if order == 0:\n",
        "            seg_old_spacing = resize_segmentation(segmentation, shape_original_after_cropping, 0, 0)\n",
        "        else:\n",
        "            if force_separate_z is None:\n",
        "                if get_do_separate_z(dct.get('original_spacing')):\n",
        "                    do_separate_z = True\n",
        "                    lowres_axis = get_lowres_axis(dct.get('original_spacing'))\n",
        "                elif get_do_separate_z(dct.get('spacing_after_resampling')):\n",
        "                    do_separate_z = True\n",
        "                    lowres_axis = get_lowres_axis(dct.get('spacing_after_resampling'))\n",
        "                else:\n",
        "                    do_separate_z = False\n",
        "                    lowres_axis = None\n",
        "            else:\n",
        "                do_separate_z = force_separate_z\n",
        "                if do_separate_z:\n",
        "                    lowres_axis = get_lowres_axis(dct.get('original_spacing'))\n",
        "                else:\n",
        "                    lowres_axis = None\n",
        "\n",
        "            print(\"separate z:\", do_separate_z, \"lowres axis\", lowres_axis)\n",
        "            seg_old_spacing = resample_data_or_seg(segmentation[None], shape_original_after_cropping, is_seg=True,\n",
        "                                                   axis=lowres_axis, order=order, do_separate_z=do_separate_z, cval=0)[\n",
        "                0]\n",
        "    else:\n",
        "        seg_old_spacing = segmentation\n",
        "\n",
        "    bbox = dct.get('crop_bbox')\n",
        "\n",
        "    if bbox is not None:\n",
        "        seg_old_size = np.zeros(shape_original_before_cropping)\n",
        "        for c in range(3):\n",
        "            bbox[c][1] = np.min((bbox[c][0] + seg_old_spacing.shape[c], shape_original_before_cropping[c]))\n",
        "        seg_old_size[bbox[0][0]:bbox[0][1],\n",
        "        bbox[1][0]:bbox[1][1],\n",
        "        bbox[2][0]:bbox[2][1]] = seg_old_spacing\n",
        "    else:\n",
        "        seg_old_size = seg_old_spacing\n",
        "\n",
        "    seg_resized_itk = sitk.GetImageFromArray(seg_old_size.astype(np.uint8))\n",
        "    seg_resized_itk.SetSpacing(dct['itk_spacing'])\n",
        "    seg_resized_itk.SetOrigin(dct['itk_origin'])\n",
        "    seg_resized_itk.SetDirection(dct['itk_direction'])\n",
        "    sitk.WriteImage(seg_resized_itk, out_fname)\n",
        "\n",
        "    sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/nnUNet/nnunet/inference/segmentation_export.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEBELq-JuQzv",
        "colab_type": "text"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wziBLg8FWoTP",
        "colab_type": "text"
      },
      "source": [
        "Now that everything is prepared you can run the actual inferece on some data. The generated attention maps are saved under  `/content/inference_results`. It might take some time for the first results to appear.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8VNvJKhYGVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/inference_results\n",
        "!nnUNet_predict -i /content/nnUNet_raw_data_base/nnUNet_raw_data/nnUNet_raw_data/Task005_Prostate/imagesTs -o /content/inference_results -t 5 -m 3d_fullres"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AumHWIukS0NT",
        "colab_type": "text"
      },
      "source": [
        "Now you can download the attention maps and inspect them (e.g. with ITK-Snap) if you want."
      ]
    }
  ]
}