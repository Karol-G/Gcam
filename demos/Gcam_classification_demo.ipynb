{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gcam classification demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC1x79crLbWG",
        "colab_type": "text"
      },
      "source": [
        "# **Using Gcam for classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "412fWaVFLi5n",
        "colab_type": "text"
      },
      "source": [
        "In this demo you will learn how to use Gcam for classification using a resnet152. We will use the famous [Cats vs Dogs Dataset](https://github.com/Karol-G/gcam_cat_dog_examples) for this demo. \\\\\n",
        "\n",
        "This demonstration was made using Google Colab and probably won't work if you are not using Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULVBcN6rNYKj",
        "colab_type": "text"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AjINkJ-Na4T",
        "colab_type": "text"
      },
      "source": [
        "Clone the Cats vs Dogs repository and set up the data structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czy4xrl0NoSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Karol-G/gcam_cat_dog_examples.git\n",
        "!mkdir /content/dataset\n",
        "!mv /content/gcam_cat_dog_examples/data/test/cats /content/dataset/cats/\n",
        "!mv /content/gcam_cat_dog_examples/data/test/dogs /content/dataset/dogs/\n",
        "!rm -r /content/gcam_cat_dog_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXW6az_9Otqd",
        "colab_type": "text"
      },
      "source": [
        "Install gcam:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3QabwbOOuP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install gcam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRnRCxG6O8Ku",
        "colab_type": "text"
      },
      "source": [
        "# Model & dataloader setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51ExmwZVQky5",
        "colab_type": "text"
      },
      "source": [
        "TODO TEXT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ATUSqGPPY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "# Setup the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = models.resnet152(pretrained=True)\n",
        "model.to(device=device)\n",
        "model.eval()\n",
        "\n",
        "def load_image(image_path):\n",
        "    raw_image = cv2.imread(image_path)\n",
        "    raw_image = cv2.resize(raw_image, (224,) * 2)\n",
        "    image = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )(raw_image[..., ::-1].copy())\n",
        "    image = image.to(device)\n",
        "    return image\n",
        "\n",
        "# Load the dataset\n",
        "dataset = ImageFolder('dataset', loader=load_image)\n",
        "# Set up the dataloader\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_t16-CHQpOw",
        "colab_type": "text"
      },
      "source": [
        "# Injecting Gcam into resnet152"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYHTpfmjQt_L",
        "colab_type": "text"
      },
      "source": [
        "The beauty of Gcam is that you only need to insert a single line of code (or two if you count the import) for everything to work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sokv7ECZPcB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gcam import gcam\n",
        "\n",
        "model = gcam.inject(model, output_dir='attention_maps', backend='gcam', layer='layer4', save_maps=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay2eZJuSXsje",
        "colab_type": "text"
      },
      "source": [
        "After your model is injected with Gcam it will still behave as it would normally do. So even if you have a big and complex project nothing will break and it will run as it always did. \\\\\n",
        "The only difference is that every time the `model.forward()` of your model is called attention maps will be generated for your current input and automatically saved to `output_dir`. \\\\\n",
        "The output of your model stays the same as before the injection. (Of course you can change this behavior and return the attention maps instead by setting `replace=True` during the injection).\n",
        "\n",
        "Now to generate some attention maps we will call the `model.forward()` with the cat and dog images a couple times:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obfr3Xk8aAAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, batch in enumerate(data_loader):\n",
        "    if i == 10:\n",
        "        break\n",
        "    _ = model(batch[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nst34bUteTYk",
        "colab_type": "text"
      },
      "source": [
        "Now you can display the generated attention maps in colab with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEsgyHFKeaO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image('/content/attention_maps/layer4/attention_map_0_0_0.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0yxK-7Tep0R",
        "colab_type": "text"
      },
      "source": [
        "# Some further notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTJB4FCbRuDD",
        "colab_type": "text"
      },
      "source": [
        "As Gcam offers multiple methods of visualization (*backends*) you can simply change the backend keyword to one of the following: \\\\\n",
        "- *gbp* (Guided Backpropagation)\n",
        "- *gcam* (Grad-Cam, default)\n",
        "- *ggcam* (Guided Grad-Cam)\n",
        "- *gcampp* (Grad-Cam++)\n",
        "\n",
        "The layer keyword tells Gcam for which layer the attention maps should be generated. You can also set the layer to 'auto' (the default setting) and Gcam will choose the last convolutional layer which is what you want in most cases. However this is still experimental and won't always choose the correct layer. But it works in most cases. \\\\\n",
        "You can print all layers of a model with `gcam.get_layers(model)` if you don't know the layer names. However you cannot generate attention maps from every layer."
      ]
    }
  ]
}